course evaluation
https://fha8.de/oN1Yx
1 / 88
Principles of Machine Learning
Prof. Christian Bauckhage
2 / 88
lecture 14
k-means clustering, GMMs, and the EM algorithm
in this lecture, we first briefly recall the idea of k-means clustering and discuss
several algorithms to compute it
we then point out implicit limitations of k-means clustering which practitioners
should (or must) be aware of
these then lead us to two main topics of this lecture, namely Gaussian mixture
models (GMMs) and the expectation maximization (EM) algorithm
3 / 88
outline
data clustering
k-means clustering
Lloydâ€™s algorithm
Hartigansâ€™s algorithm
MacQueenâ€™s algorithm
characteristics of k-means
Gaussian mixture models
the expectation maximization algorithm
summary
4 / 88
data clustering
5 / 88
clustering
â‡” given a finite data set D, automatically identify latent structures
or groups of similar data points within D
â†’
6 / 88
note
in general, clustering is an ill-posed problem
7 / 88
example
how to cluster this set of objects ?
are the possible clusters unique ?
8 / 88
note
there are many clustering philosophies and algorithms
hierarchical clustering
divisive or agglomerative linkage methods, . . .
(cor)relational clustering
spectral clustering, graph cuts, . . .
density-based clustering
DBSCAN, ITVQ, . . .
prototype-based clustering
k-means, k-medoids, LBG, . . .
9 / 88
note
differences between methods boil down to how they answer these questions
Q1: what defines similarity of data points ?
Q2: what properties should a cluster have ?
10 / 88
k-means clustering
11 / 88
k-means clustering
is the â€œmost popularâ€ prototype-based clustering approach for vector valued data
D =

x1, . . . , xn
	
âŠ‚ R
m
12 / 88
k-means clustering
determines k â‰ª n clusters Ci and answers Q2 as follows
Ci âŠ‚ D, Ci âˆ© Cj = âˆ… âˆ€ i Ì¸= j,
[
k
i=1
Ci = D
considers cluster centroids Âµi
to answer Q1 as follows
Ci =


x âˆˆ D




 x âˆ’ Âµi


2
â©½

 x âˆ’ Âµj


2
âˆ€ i Ì¸= j

Âµi =
1
|Ci
|
X
xâˆˆCi
x
13 / 88
note
â‡’ the essential problem of k-means clustering is to find k suitable centroids Âµ1, . . . , Âµk
once the user has specified k, this can be done by minimizing an objective function
E

C1, C2, . . . , Ck

= Ek
there are various possible formulations, however . . .
14 / 88
k-means objective function
k-means is most commonly achieved through minimizing
Ek =
X
k
i=1
X
xjâˆˆCi

 xj âˆ’ Âµi


2
=
X
k
i=1
Xn
j=1
zij

 xj âˆ’ Âµi


2
(1)
where we introduced additional binary indicator variables
zij =

1, if xj âˆˆ Ci
0, otherwise
(2)
15 / 88
note
the problem of solving
argmin
Âµ1,...,Âµk
Ek
looks innocent but is actually NP-hard
(in the exercise, we already saw that k-means clustering is
an integer programming problem in disguise â‡” NP-hard)
â‡’ Ek has numerous local minima and there exists no algorithm
known today that is guaranteed to find the optimal solution
16 / 88
principles of machine learning
â‡’ we have
any known algorithm for k-means clustering is a heuristic
â‡’ we realize
in practice, any algorithm for k-means clustering should be
run several times to empirically determine the best solution
17 / 88
Lloydâ€™s algorithm (aka the k-means algorithm)
t â† 0
â€œsomehowâ€ initialize Âµ1, Âµ2, . . . , Âµk
repeat
for i = 1, . . . , k // update clusters
Ci =


x âˆˆ D




 x âˆ’ Âµi


2
â©½

 x âˆ’ Âµj


2

for i = 1, . . . , k // update centroids
Âµi =
1
|Ci
|
X
xâˆˆCi
x
t â† t + 1
until convergence
18 / 88
possible convergence criteria
centroids stabilize

Âµi
[t] âˆ’ Âµi
[t âˆ’ 1]

 â©½ Ïµ âˆ€ i
clusters stabilize
Ci
[t] âˆ© Ci
[t âˆ’ 1] = Ci
[t] âˆ€ i
iterations exceed a threshold
t > tmax
19 / 88
example
initialization 1st update 2nd update
Â· Â· Â·
final result
20 / 88
note
Lloydâ€™s algorithm is guaranteed to converge, because
1) since
Âµi = argmin
ÂµâˆˆRm
X
xâˆˆCi

x âˆ’ Âµ


2
the centroid updates cannot increase Ek
2) by design the cluster updates cannot increase Ek
â‡’ we have 0 â©½ Ek[t + 1] â©½ Ek[t] so that Lloydâ€™s algorithm converges to a (local) minimum
21 / 88
note
the fact that Lloydâ€™s algorithm will converge says nothing about the quality
of the solution
the fact that Lloydâ€™s algorithm will converge says nothing about the speed
of convergence
in fact, it usually converges quickly but its quality crucially depends on the
initialization of the means Âµ1, Âµ2, . . . , Âµk
www.youtube.com/watch?v=5I3Ei69I40s
www.youtube.com/watch?v=9nKfViAfajY
22 / 88
Hartiganâ€™s algorithm
for all
x
âˆˆ
D
randomly assign
x to a cluster
C
i
for all
C
i
âˆˆ

C
1, . . . ,
Ck
	
compute cluster centroid
Âµ
i
repeat
converged
â† True
for all
x
âˆˆ
D
determine
C
i
=
C
(
x
)
remove
x from
C
i
recompute
Âµ
i
determine Cw = argmin
Cj
E

C
1, . . . ,
Cj
âˆª
{x
}, . . . ,
Ck

if
Cw
Ì¸=
C
i
converged
â† False
assign
xj
to
Cw
recompute
Âµ
w
until converged
23 / 88
example
random initialization result after one outer loop result after two outer loops
24 / 88
Hartiganâ€™s algorithm
is much less well known than Lloydâ€™s algorithm
is provably more robust than Lloydâ€™s algorithm
is slow and a bit cumbersome to implement
provably converges
converges quickly
www.youtube.com/watch?v=ivr91orblu8
25 / 88
MacQueenâ€™s algorithm
initializeÂµ
1, Âµ
2, . . . ,
Âµ
k
	
=

x
1, x
2, . . . ,
x
k
	

n
1, n
2, . . . ,
n
k
	
=

1, 1, . . . ,
1
	
for all
x
âˆˆ

x
k
+
1, x
k
+
2, . . . ,
x
n
	
determine winner centroid Âµw = argmin i  x âˆ’ Âµi2
update cluster size and centroid nw â† nw + 1 Âµw â† Âµw + 1nw  x âˆ’ Âµw
for all
C
i
âˆˆ

C
1, . . . ,
Ck
	
C
i
=


x
âˆˆ
D  
x
âˆ’
Âµ
i

2
â©½

x
âˆ’
Âµ
l

2

26 / 88
example
after 3 points after 4 points after 5 points after 6 points
after 100 points after 150 points after 200 points after 250 points
27 / 88
MacQueenâ€™s algorithm
is also known as online k-means
exploits that sample means can be computed recursively
Âµ
(n) =
1
n
Xn
j=1
xj =
1
n
Xnâˆ’1
j=1
xj +
1
n
xn =
n âˆ’ 1
n
Âµ
(nâˆ’1) +
1
n
xn
applies to situations where |D| â‰« 1 or where the data arrive one at a time
www.youtube.com/watch?v=hzGnnx0k6es
28 / 88
characteristics of k-means
29 / 88
k-means clustering works well if the data consist of k isotropic,
well separated Gaussian blobs
â†’
30 / 88
it also works for non-isotropic but still well separated Gaussian blobs
â†’
31 / 88
if the data consist of isotropic but not so well separated blobs,
k-means results are not as â€œplausibleâ€ as one would hope for
â†’
32 / 88
serious problems arise for non-isotropic nearby blobs
â†’
33 / 88
if the data do not consist of Gaussian blobs, we cannot
expect k-means to produce cognitively plausible results
â†’
34 / 88
question
why is this ?
answer
letâ€™s see . . .
35 / 88
Gaussian mixture models
36 / 88
a probabilistic view on clustering
imagine the given xj âˆˆ D were produced as follows
sample a cluster Ci according to a discrete probability p

Ci

sample a point xj according to a continuous conditional probability p

xj

 Ci

under this generative model, the probability for observing a certain xj
is given by
p

xj

=
X
k
i=1
p

xj

 Ci

p

Ci

37 / 88
two modeling assumptions
1) let the elements in each cluster be distributed according to
p

x

 Ci

= N

x

 Âµi
, Î£i

= Î³i e
âˆ’ 1
2
(xâˆ’Âµi)
âŠºÎ£âˆ’1
i
(xâˆ’Âµi)
i.e. a multivariate Gaussian with normalization constant Î³i
2) let Î£i = I (each Gaussian is isotropic and of unit variance)
p

x

 Ci

= N

x

 Âµi

= (2 Ï€)
âˆ’ m
2 e
âˆ’ 1
2
âˆ¥xâˆ’Âµiâˆ¥
2
38 / 88
â‡’ writing wi = p

Ci

, we have a simple Gaussian mixture model (GMM)
p

x

 Î¸

=
X
k
i=1
wi N

xj

 Âµi

whose parameters are but weights and means
Î¸ =

w1, Âµ1, . . . , wk, Âµk

39 / 88
likelihood and log-likelihood
L

Î¸

 D

= p

D

 Î¸

=
Yn
j=1
p

xj

 Î¸

=
Yn
j=1
X
k
i=1
wi N

xj

 Âµi

L

Î¸

 D

= log p

D

 Î¸

=
Xn
j=1
log p

xj

 Î¸

=
Xn
j=1
log "X
k
i=1
wi N

xj

 Âµi

#
40 / 88
note
the above log-likelihood is rather unwieldy (a sum over logarithms of sums) so that
the recipe of considering âˆ‡L = 0 does not lead to a closed form solution, but . . .
a great idea due to Dempster et al. (1977) is to assume a set of indicator variables
Z =

z11,z12, . . . ,zkn	
just as introduced in (2) and to consider . . .
41 / 88
complete likelihood
L

Î¸

 D, Z

=
Yn
j=1
X
k
i=1
zij wi N

xj

 Âµi

42 / 88
note
since
zij âˆˆ

0, 1
	
X
k
i=1
zij = 1
i.e. since each xj belongs to only one Ci
, we can write
p

xj

 Î¸

=
X
k
i=1
zij wi N

xj

 Âµi

=
Y
k
i=1
h
wi N

xj

 Âµi

izij
this is an incredibly important trick !!!
43 / 88
complete log-likelihood
L

Î¸

 D, Z

=
Xn
j=1
X
k
i=1
zijh
log wi + log N

xj

 Âµi

i
=
Xn
j=1
X
k
i=1
zijh
log wi + log(2 Ï€)
âˆ’ m
2 âˆ’
1
2

xj âˆ’ Âµi


2
i
=
Xn
j=1
X
k
i=1
zij log wi
| {z }
T1
+
Xn
j=1
X
k
i=1
zij log(2 Ï€)
âˆ’ m
2
| {z }
T2
âˆ’
1
2
Xn
j=1
X
k
i=1
zij

xj âˆ’ Âµi


2
| {z }
T3
44 / 88
note
maximizing L

Î¸

 D, Z

= T1 + T2 âˆ’
1
2
T3 requires minimizing T3
however, looking at
T3 =
X
k
i=1
Xn
j=1
zij

xj âˆ’ Âµi


2
we recognize the common k-means minimization objective
45 / 88
principle of machine learning
â‡’ we realize that
k-means clustering implicitly fits an isotropic GMM
46 / 88
question
what about non-isotropic GMMs ?
can we fit them to a given data set ?
can we estimate their parameters ?
answer
letâ€™s see . . .
47 / 88
the expectation maximization algorithm
48 / 88
given D =

x1, . . . xn
	
âŠ‚ R
m, we now consider a
general multivariate Gaussian mixture model
p

x

 Î¸

=
X
k
i=1
wi N

x

 Âµi
, Î£i

with mixture components and weights such that
N

x

 Âµi
, Î£i

=
e
âˆ’ 1
2
[xâˆ’Âµi]
âŠºÎ£âˆ’1
i
[xâˆ’Âµi]
p
(2 Ï€)
m det Î£i
wi â©¾ 0
X
k
i=1
wi = 1
49 / 88
for parameter estimation, we assume independence
p

D

 Î¸

=
Yn
j=1
p

xj

 Î¸

=
Yn
j=1
X
k
i=1
wi N

xj

 Âµi
, Î£i

so that the corresponding log-likelihood amounts to
L

Î¸

=
Xn
j=1
log "X
k
i=1
wi N

xj

 Âµi
, Î£i

#
(3)
instead of L

Î¸

 D

, we henceforth write L

Î¸

for brevity
50 / 88
(3) is once again â€œnastyâ€ since it involves logarithms of sums
our parameter estimation problem would be once again much
easier, if we knew which mixture component Ci
is responsible
for observation xj
because if we knew this, the joint probability would simplify to
p

D

 Î¸

=
Yn
j=1
wi N

xj

 Âµi
, Î£i

and the log-likelihood would become
L

Î¸

=
Xn
j=1
logh
wi N

xj

 Âµi
, Î£i

i
51 / 88
â‡” if we knew which component Ci had generated observation xj
,
we could group the given data into sets D = D1 âˆª . . . âˆª Dk and
treat our overall problem in terms of k independent problems
simple MLEs of the parameters of each component would be
wi =
|Di
|
|D|
=
ni
n
Âµi =
1
ni
X
xjâˆˆDi
xj
Î£i =
1
ni
X
xjâˆˆDi

xj âˆ’ Âµi
xj âˆ’ Âµi
âŠº
convince yourself that this is true
52 / 88
note
we already know that if we knew which component Ci had generated observation xj
,
we can express this knowledge in terms of indicator variables Z =

zij	
where
zij =

1, if xj âˆˆ Ci
0, otherwise
we also already know that we can then write
p

D, Z

 Î¸

=
Yn
j=1
Y
k
i=1
h
wi N

xj

 Âµi
, Î£i

izij
53 / 88
note
using indicator variables zij, the parameters of
each mixture component can be computed as
ni =
Xn
j=1
zij
wi =
ni
n
Âµi =
1
ni
Xn
j=1
zij xj
Î£i =
1
ni
Xn
j=1
zij
xj âˆ’ Âµi
xj âˆ’ Âµi
âŠº
convince yourself that this is true
54 / 88
note
unfortunately, we generally do not know which component Ci caused
observation xj âˆˆ D, however . . .
we can treat the Z =

zij	
as latent or unobserved variables such that
p

D

 Î¸

=
X
Z
p

D, Z

 Î¸

read P
Z
. . . as â€œsum over all possible instantiations of the zijâ€
55 / 88
disclaimers
note that the expression
p

D

 Î¸

=
X
Z
p

D, Z

 Î¸

is model agnostic, i.e. hides the fact that we want to fit a GMM
in what follows, we will keep working with general expressions
what follows might seem crazy and you might be wondering where it comes from
to make a long story short, it comes from the minds of Dempster, Laird, and Rubin
A.P. Dempster, N.M. Laird, and D.B. Rubin, â€œMaximum Likelihood from Incomplete Data via the EM
Algorithmâ€, J. Royal Statistical Society B, 39(1), 1977
now, letâ€™s switch gears . . .
56 / 88
we may assume that the hidden variables are distributed
according to â€œsomeâ€ q

Z

and consider the log-likelihood
L

Î¸

= logX
Z
p

D, Z

 Î¸

= logX
Z
q

Z

q

Z
 p

D, Z

 Î¸

= logX
Z
q

Z
 p

D, Z

 Î¸

p

Z

using Jensenâ€™s inequality â©¾
X
Z
q

Z

log
p

D, Z

 Î¸

q

Z

57 / 88
ELBO
the expression
J

q, Î¸

=
X
Z
q

Z

log
p

D, Z

 Î¸

q

Z

is called evidence lower bound (ELBO)
this ELBO is, in fact, an expectation
J

q, Î¸

= Eq(Z)
"
log
p

D, Z

 Î¸

q

Z

#
58 / 88
observe
we want to estimate model parameters Î¸ which maximize L

Î¸

we just found a lower bound J

q, Î¸

â©½ L

Î¸

for this log-likelihood
â‡” if we maximize the ELBO J

q, Î¸

, we automatically maximize L

Î¸

59 / 88
observe
the inequality
J

q, Î¸

â©½ L

Î¸

holds for any valid distribution q

Z

and we did not specify it yet
so, which q

Z

could we choose to make the lower bound tight ?
and what we really want to estimate are the parameters Î¸, right ?
here is an idea . . .
60 / 88
iterative alternating algorithm
t â† 0
guess Î¸t
repeat
maximize J

q, Î¸t

w.r.t. q
qt = argmax
q
J

q, Î¸t

maximize J

qt
, Î¸

w.r.t. Î¸
Î¸t+1 = argmax
Î¸
J

qt
, Î¸

t â† t + 1
until

Î¸t âˆ’ Î¸tâˆ’1

 â©½ Ïµ
61 / 88
observe
the above procedure resembles Lloydâ€™s algorithm for k-means !
question
but q is a distribution . . . how do we maximize w.r.t. a distribution ?
answer
well . . .
62 / 88
we can rewrite the ELBO as
J

q, Î¸

= Eq(Z)
"
log
p

D, Z

 Î¸

q

Z

#
= Eq(Z)
"
log
p

Z

 D, Î¸

p

D

 Î¸

q

Z

#
= Eq(Z)
"
log p

D

 Î¸

+ log
p

Z

 D, Î¸

q

Z

#
= Eq(Z)
"
log p

D

 Î¸

âˆ’ log
q

Z

p

Z

 D, Î¸

#
= log p

D

 Î¸

âˆ’ Eq(Z)
"
log
q

Z

p

Z

 D, Î¸

#
= L

Î¸

âˆ’ DKL
q

Z

âˆ¥ p

Z

 D, Î¸


63 / 88
Kullback-Leibler divergence
the quantity
DKL
q

Z

âˆ¥ p

Z

 D, Î¸


is a Kullback-Leibler divergence
for KL-divergences, we always have
DKL
q âˆ¥ p

â©¾ 0
DKL
q âˆ¥ p

= 0 â‡” q = p
64 / 88
â‡’ choosing
q

Z

= p

Z

 D, Î¸

(4)
we obtain
J

q, Î¸

= L

Î¸

âˆ’ DKL
p

Z

 D, Î¸

âˆ¥ p

Z

 D, Î¸


| {z }
=0
â‡” using (4), we have
J

q, Î¸

= L

Î¸

â‡” using (4), the lower bound J

q, Î¸

â©½ L

Î¸

becomes tight
65 / 88
we can also rewrite the ELBO as
J

q, Î¸

= Eq(Z)
"
log
p

D, Z

 Î¸

q

Z

#
= Eq(Z)
h
log p

D, Z

 Î¸

âˆ’ log q

Z

i
= Eq(Z)
h
log p

D, Z

 Î¸

i
âˆ’ Eq(Z)
h
log q

Z

i
= Eq(Z)
h
log p

D, Z

 Î¸

i
âˆ’ H

q

where the (information) entropy H

q

is independent of the model parameters Î¸
66 / 88
if we plug our choice q

Z

= p

Z

 D, Î¸

into the above, we obtain
J

q, Î¸

= Ep(Z|D,Î¸)
h
log p

D, Z

 Î¸

i
+ cnst
people also like to write the first term on the right as
Q

Î¸

 Î¸t

= Ep(Z|D,Î¸t)
h
log p

D, Z

 Î¸

i
=
X
Z
p

Z

 D, Î¸t

log p

D, Z

 Î¸

with the understanding that Î¸t denotes our current guess of the parameters Î¸
67 / 88
expectation
the expression
Q

Î¸

 Î¸t

= Ep(Z|D,Î¸t)
h
log p

D, Z

 Î¸

i
=
X
Z
p

Z

 D, Î¸t

log p

D, Z

 Î¸

(E)
is the expected value of the complete log-likelihood L

Î¸

 D, Z

w.r.t. the current
conditional distribution of Z given the data D and the current estimates Î¸t of the
sought after model parameters
68 / 88
maximization
we can use it to update our current guess of the model parameters
Î¸t+1 = argmax
Î¸
Q

Î¸

 Î¸t

(M)
all in all, this leads to the . . .
69 / 88
EM algorithm
t â† 0
guess Î¸t
repeat
E-step: compute
Q

Î¸

 Î¸t

= Ep(Z|D,Î¸t)
h
log p

D, Z

 Î¸

i
M-step: compute
Î¸t+1 = argmax
Î¸
Q

Î¸

 Î¸t

t â† t + 1
until

Î¸t âˆ’ Î¸tâˆ’1

 â©½ Ïµ
now we need to tie this back to GMMs
70 / 88
towards EM for GMMs
we have the complete likelihood
L

Î¸

 D, Z

= p

D, Z

 Î¸

=
Yn
j=1
Y
k
i=1
h
wi N

xj

 Âµi
, Î£i

izij
and thus the complete log-likelihood
L

Î¸

 D, Z

= log p

D, Z

 Î¸

=
Xn
j=1
X
k
i=1
zijh
log wi + log N

xj

 Âµi
, Î£i

i
71 / 88
â‡’ we have the expectation
Ep(Z|D,Î¸)
h
log p

D, Z

 Î¸

i
= Ep(Z|D,Î¸)
ï£®
ï£°
Xn
j=1
X
k
i=1
zijh
log wi + log N

xj

 Âµi
, Î£i

i
ï£¹
ï£»
=
Xn
j=1
X
k
i=1
Ep(Z|D,Î¸)
h
ziji hlog wi + log N

xj

 Âµi
, Î£i

i
moreover, since zij âˆˆ {0, 1}, we simply have
Ep(Z|D,Î¸)
h
ziji
= 0 Â· p

zij = 0

 xj
, Î¸i

+ 1 Â· p

zij = 1

 xj
, Î¸i

= p

zij = 1

 xj
, Î¸i

72 / 88
we further have
p

zij = 1

 xj
, Î¸i

=
p

zij = 1, xj

 Î¸i

p

xj

 Î¸i
 =
wi N

xj

 Âµi
, Î£i

P
k
l=1
wl N

xj

 Âµl
, Î£l

â‰¡ Î¶ij
â‡’ for a GMM, we have
Ep(Z|D,Î¸)
h
log p

D, Z

 Î¸

i
=
Xn
j=1
X
k
i=1
Î¶ij h
log wi + log N

xj

 Âµi
, Î£i

i
73 / 88
EM for GMMs
for i = 1, . . . , k
guess wi
, Âµi
, Î£i
repeat
for i = 1, . . . , k // E-step
for j = 1, . . . , n
Î¶ij =
wi N

xj

 Âµi
, Î£i

Pk
l=1
wl N

xj

 Âµl
, Î£l

for i = 1, . . . , k // M-step
w
new
i
, Âµ
new
i
, Î£new
i = argmax
wi
,Âµi
,Î£i
X
i
X
j
Î¶ij h
log wi + log N

xj

 Âµi
, Î£i

i
until convergence
74 / 88
note
we know from earlier lectures that there exist closed form MLEs for the wi
, Âµi
, Î£i
namely . . .
75 / 88
to determine the optimal Âµl
, we consider
âˆ‚
âˆ‚Âµl
X
i
X
j
h
Î¶ij log N

xj

 Âµi
, Î£i

+ Î¶ij log wi
i
=
âˆ‚
âˆ‚Âµl
X
j
Î¶lj log N

xj

 Âµl
, Î£l

=
âˆ‚
âˆ‚Âµl
X
j
Î¶lj log e
âˆ’ 1
2
[xjâˆ’Âµl
]
âŠºÎ£âˆ’1
l
[xjâˆ’Âµl
]
p
(2Ï€)
m det Î£l
| {z }
=Î³l
=
âˆ‚
âˆ‚Âµl
X
j
âˆ’Î¶lj
1
2

xj âˆ’ Âµl
âŠºÎ£âˆ’1
l

xj âˆ’ Âµl

âˆ’ Î¶lj log Î³l
=
âˆ‚
âˆ‚Âµl
X
j
âˆ’Î¶lj
1
2

xj âˆ’ Âµl
âŠºÎ£âˆ’1
l

xj âˆ’ Âµl

=
X
j
Î¶ljh
Î£âˆ’1
l
xj âˆ’ Î£âˆ’1
l Âµl
i
= Î£âˆ’1
l
X
j
Î¶ljh
xj âˆ’ Âµl
i
76 / 88
equating this to 0 and solving for Âµl
, yields
Âµl =
P
j
Î¶lj xj
P
j
Î¶lj
similarly, we find for the covariance matrix Î£l
Î£l =
P
j
Î¶lj
xj âˆ’ Âµl
xj âˆ’ Âµl
âŠº
P
j
Î¶lj
77 / 88
when updating the mixture coefficients wl
, we must pay
attention to the constraint
X
i
wi = 1
we thus introduce a Lagrange parameter Î» and consider
âˆ‚
âˆ‚wl
"X
i
X
j
h
Î¶ij log N

xj

 Âµi
, Î£i

+ Î¶ij log wi
i
+ Î»
X
i
wi âˆ’ 1

#
78 / 88
we find
âˆ‚
âˆ‚wl
ï£®
ï£°
X
i
X
j
h
Î¶ij log N

xj

 Âµi
, Î£i

+ Î¶ij log wi
i
+ Î»
X
i
wi âˆ’ 1

ï£¹
ï£» =
X
j
Î¶lj
wl
+ Î»
equating to 0 and solving for wl yields
wl =
P
j
Î¶lj
âˆ’Î»
plugging this into the constraint yields
X
i
wi = âˆ’
1
Î»
X
i
X
j
Î¶ij = 1 â‡” âˆ’Î» =
X
i
X
j
Î¶ij
79 / 88
â‡’ the updated mixture weights are
wl =
P
j
Î¶lj
P
i
P
j
Î¶ij
=
P
j
Î¶lj
n
just to be clear
X
i
X
j
Î¶ij =
X
j
X
i
p

zij = 1

 xj
,Î¸i

=
X
j
1
= n
80 / 88
examples
next, we look at the practical behavior of EM for GMM fitting
in the first two experiments, we set k = 3 and observe that
the EM procedure may work really well if k coincides with the number of components
actually present in the data
alas, good results can not be guaranteed; rather the behavior of the algorithm depends
on the initial guesses for the parameters to be estimated; EM might get stuck in a local
optimum and not find the overall best solution
in the third experiment, we set k = 7 and thus try to fit more
components than the data contains; we observe that
EM happily fits whatever model we ask it to fit; whether or not the model makes sense
cannot be decided by the algorithm
81 / 88
experiment 1, k = 3
good initialization leads to good results
t = 0 t = 1 t = 2 t = 3 t = 4 t = 5
82 / 88
experiment 2, k = 3
bad initialization leads to bad results
t = 0 t = 1 t = 2 t = 3 t = 4 t = 5
83 / 88
experiment 3, k = 7
EM will fit any model, appropriate or not
t = 0 t = 1 t = 2 t = 3 t = 4 t = 5
84 / 88
note
the EM algorithm
is an incredibly important tool
+ is guaranteed to converge
+ tends to work well in practice
+ works with many distributions (not just with Gaussians)
+ woks with MAP estimators, too (not just with MLEs)
âˆ’ may not find the global maximum / optimal parameters
âˆ’ is generally rather slow
85 / 88
summary
86 / 88
we now know about
k-means clustering
the fact that it implicitly fits a GMM and is
therefore tailored to locally Gaussian data
the fact that it is a difficult problem (NP-hard)
whose optimal solution cannot be guaranteed
the fact that there exist several k-means heuristics
Lloydâ€™s algorithm
Hartiganâ€™s algorithm
MacQueenâ€™s algorithm
87 / 88
we now know about
Gaussian mixture models (GMMs)
the expectation maximization (EM) algorithm
the ELBO and its pivotal role in the EM algorithm
note: the ELBO occurs in other ML settings, too
88 / 88