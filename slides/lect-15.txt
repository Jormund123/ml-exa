Principles of Machine Learning
Prof. Christian Bauckhage
1 / 89
lecture 15
ITVQ, reproducing kernel Hilbert spaces, and mean discrepancy based VQ
this final lecture is extremely dense as it first extends and then ties together a
lot of the material we studied earlier (it really is the grand finale of this course)
we first look at density models or data density estimation with Parzen windows
we then move on to vector quantization based on information theoretic criteria
next, we glimpse at the theory of reproducing kernel Hilbert spaces (RKHSs)
we then finally look at RBF density modeling and vector quantization from the
point of view of RKHSs and the mean discrepancy between two distributions
2 / 89
outline
recap and a simple new idea
information theoretic vector quantization
reproducing kernel Hilbert spaces
(maximum) mean discrepancy
MD for vector quantization
summary
3 / 89
recap and a simple new idea
4 / 89
a common problem
given data
D =

x1, x2, . . . , xn
	
where the
xi âˆˆ R
m
represent certain observations as to the world, we might want to
estimate a probability density function p

x

that models how the
observations are distributed
5 / 89
last time, we used the EM algorithm to fit GMMs
p

x

=
X
k
i=1
wi N

x

 Âµi
, Î£i

but there also exist much simpler baseline ideas . . .
6 / 89
idea (akin to ideas known from lecture 02)
model the pdf as an RBF mixture
p

x

=
Xn
j=1
Ï•

x, xj

wj
and just set the mixture weights to
wj =
1
n
7 / 89
note
for this simple idea to produce a legitimate pdf, the RBF kernel must obey
Ï•

x, xj

â©¾ 0
Z
Ï•

x, xj

dx = 1
because then R
p

x

dx =
R
1
n
Pn
j=1 Ï•

x, xj

= 1
â‡’ a popular choice is the properly normalized isotropic Gaussian distribution
Ï•Ïƒ

x, xj

=
1

2 Ï€ Ïƒ2
m/2
exp 
âˆ’
âˆ¥x âˆ’ xjâˆ¥
2
2 Ïƒ2

note the extended notation Ï•Ïƒ

x, xj

8 / 89
examples (1D)
using the following Gaussian basis distributions
Ïƒ = 1/4 Ïƒ = 1/2
Ïƒ = 1 Ïƒ = 2
9 / 89
yields the following density estimates
Ïƒ = 1/4 Ïƒ = 1/2
Ïƒ = 1 Ïƒ = 2
10 / 89
examples (2D)
Ïƒ = 1/4 Ïƒ = 1/2
Ïƒ = 1 Ïƒ = 2
11 / 89
Parzen window method
this approach to density modeling is called the
Parzen window method
(some people call it the Parzen-Rosenblatt method)
note: for BIG DATA (n â‰« 1), it is inefficient
â‡” requires working with n densities
this unfortunate issue leads us to . . .
12 / 89
information theoretic vector quantization
13 / 89
vector quantization
vector quantization is to represent a data set
X =

x1, . . . , xn
	
âŠ‚ R
m
in terms of a codebook of k â‰ª n prototypes
W =

w1, . . . , wk
	
âŠ‚ R
m
14 / 89
information theoretic vector quantization (ITVQ)
ITVQ is vector quantization based on information theory
â‡” ITVQ considers entropy and divergences of distributions
the particular approach we study next is due to Principe
and colleagues
Jose C. Principe
15 / 89
idea
given data X and an initial codebook W
1) estimate their respective distributions, i.e. assume that
X âˆ¼ p
W âˆ¼ q
2) iteratively adjust / optimize the codebook such that the
Cauchy-Schwartz divergence
DCS
p âˆ¥ q

= 2 H

p, q

âˆ’ H

q

âˆ’ H

p

between the two distributions p and q becomes small
16 / 89
example
random initial codebook W âŠ‚ X optimized codebook WË†
17 / 89
divergence
a divergence D is a function that measures how much two
probability distributions p and q differ
for any p and q defined over the same domain, we require
D

p âˆ¥ q

â©¾ 0
D

p âˆ¥ q

= 0 â‡” p = q
divergences are a dime a dozen (e.g. Kullback-Leibler divergence, Jensen-Shannon
divergence, Jeffreys divergence, . . . )
divergences arenâ€™t distances (are neither symmetric nor obey triangle inequalities)
18 / 89
again, ITVQ considers the Cauchy-Schwartz divergence
DCS
p âˆ¥ q

= 2 H

p, q

âˆ’ H

q

âˆ’ H

p

which is defined in terms of several entropy measures H
note: as H

p

is independent of q,
we henceforth ignore it
in the context of information theory, entropy measures the
amount of information required to specify the (micro) state
of a system
(letâ€™s leave it at this ;-)
19 / 89
entropy measures are a dime a dozen, too
the Cauchy-Schwartz divergence involves
Renyiâ€™s cross entropy between p and q
H

p, q

= âˆ’ ln Z
p

x

q

x

dx
Renyiâ€™s quadratic entropy of q
H

q

= âˆ’ ln Z
q
2

x

dx
20 / 89
disclaimer
for brevity, we may henceforth write
V

p, q

â‰¡
Z
p

x

q

x

dx
V

q

â‰¡
Z
q
2

x

dx
in particular, we henceforth let
c â‰¡
n V(p, q)
k V(q)
21 / 89
back to ITVQ
Principe et al. propose to model p and q in terms of Parzen estimates
p

x

=
1
n
Xn
j=1
Ï•Î¾

x, xj

q

x

=
1
k
X
k
i=1
Ï•Ï‰

x, wi

where Ï•Î¾ and Ï•Ï‰ are Gaussian density kernels of scales Î¾
2 and Ï‰2
22 / 89
using these and the Gaussian product theorem, we find
Z
p

x

q

x

dx =
1
n k
Xn
j=1
X
k
i=1
Ï•Ï„

xj
, wi

Z
q
2

x

dx =
1
k
2
X
k
l=1
X
k
i=1
Ï•Ï

wl
, wi

where
Ï„
2 = Î¾
2 + Ï‰2
Ï
2 = 2 Ï‰2
23 / 89
henceforth, we use the same bandwidth Ïƒ for both kernels
using the above results, the Cauchy-Schwartz divergence
DCS
p âˆ¥ q

= âˆ’2 ln Z
p

x

q

x

dx + ln Z
q
2

x

dx
becomes
DCS
p âˆ¥ q

= âˆ’2 ln"
1
n k
Xn
j=1
X
k
i=1
Ï•Ïƒ

xj
, wi

#
+ ln"
1
k
2
X
k
l=1
X
k
i=1
Ï•Ïƒ

wl
, wi

#
24 / 89
â‡’ differentiating DCS w.r.t. wu, equating to 0, and rearranging the result
yields the following fixed-point iteration for optimizing the codebook
w
new
u =
Pn
j=1
xj Ï•Ïƒ

xj
, wu

Pn
j=1 Ï•Ïƒ

xj
, wu

âˆ’ c Â·
Pk
i=1 wi Ï•Ïƒ

wi
, wu

Pn
j=1 Ï•Ïƒ

xj
, wu

+ c Â·
Pk
i=1 Ï•Ïƒ

wi
, wu

Pn
j=1 Ï•Ïƒ

xj
, wu
 wu
25 / 89
the previous slide showed how to update a single codebook vector
however, we can also update all codebook vectors simultaneously . . .
we introduce matrices and vectors
X =

x1 x2 Â· Â· Â· xn

âˆˆ R
mÃ—n
W =

w1 w2 Â· Â· Â· wk

âˆˆ R
mÃ—k
Ï•x

wu

=

Ï•Ïƒ

x1, wu

Â· Â· Â· Ï•Ïƒ

xn, wu
âŠº
âˆˆ R
n
Ï•w

wu

=

Ï•Ïƒ

w1, wu

Â· Â· Â· Ï•Ïƒ

wk, wu
âŠº
âˆˆ R
k
and let 1n âˆˆ R
n and 1k âˆˆ R
k denote vectors of all ones
26 / 89
this way, we can also write the update for vector wu as
w
new
u =
X Ï•x(wu)
1
âŠº
n Ï•x(wu)
âˆ’ c Â·
W Ï•w(wu)
1
âŠº
n Ï•x(wu)
+ c Â·
1
âŠº
k Ï•w(wu)
1
âŠº
n Ï•x(wu)
wu
27 / 89
if we further introduce an n Ã— k matrix
Î¦xw =
h
Ï•x(w1)Â· Â· Â· Ï•x(wk)
i
together with three k Ã— k matrices
Î¦ww =
h
Ï•w(w1)Â· Â· Â· Ï•w(wk)
i
Dxw = diag
1
âŠº
nÎ¦xw
Dww = diag
1
âŠº
k Î¦ww
then . . .
28 / 89
we can update all codebook vectors at once using
Wnew =
h
X Î¦xw âˆ’ c Â· W Î¦ww + c Â· W Dwwi
D
âˆ’1
xw
=
h
X Î¦xw + c Â· W

Dww âˆ’ Î¦ww
i
D
âˆ’1
xw
we now also have c =
n 1
âŠº
n Î¦xw 1n
k 1
âŠº
k Î¦ww 1k
29 / 89
ITVQ algorithm
initialize
W =

w1 w2 Â· Â· Â· wk

for t = 1, . . . , tmax
compute
Î¦xw, Î¦ww, Dxw, Dww, c
update
Wnew =
h
X Î¦xw + c Â· W

Dww âˆ’ Î¦ww
i
D
âˆ’1
xw
30 / 89
example
âˆ’4 âˆ’2 0 2 4 6 8
âˆ’2
âˆ’1
0
1
2
3
4
k = 5, Ïƒ = 0.5
âˆ’4 âˆ’2 0 2 4 6 8
âˆ’2
âˆ’1
0
1
2
3
4
k = 5, Ïƒ = 1.0
âˆ’4 âˆ’2 0 2 4 6 8
âˆ’2
âˆ’1
0
1
2
3
4
k = 20, Ïƒ = 0.5
âˆ’4 âˆ’2 0 2 4 6 8
âˆ’2
âˆ’1
0
1
2
3
4
k = 20, Ïƒ = 1.0
31 / 89
reproducing kernel Hilbert spaces
32 / 89
observe
above, we have been working with kernel functions k

x, xj

to model densities
previously, we learned about the kernel trick which swaps inner products x
âŠºxj
for kernel evaluations k

xi
, xj

to apply linear algorithms to non-linear settings
question
is there a connection ?
an / 89
observe
above, we have been working with kernel functions k

x, xj

to model densities
previously, we learned about the kernel trick which swaps inner products x
âŠºxj
for kernel evaluations k

xi
, xj

to apply linear algorithms to non-linear settings
question
is there a connection ?
answer
this will require some work but letâ€™s see . . .
33 / 89
disclaimer
in what follows, we mainly focus on functions f : R
m â†’ R
but most of what we discuss generalizes to other settings
to begin with, we warm up to the idea of Hilbert spaces of functions . . .
34 / 89
square integrable function
a function f : R
m â†’ R is square integrable or in class L
2
, if
Z
Rm


f(x)


2
dx < âˆ
to reduce clutter, we henceforth
drop the integration boundaries
35 / 89
examples (for m = 1)
x
f(x)
square integrable
x
f(x)
not square integrable
x
f(x)
square integrable
x
f(x)
not square integrable
36 / 89
L
2
, +, Â·

is an instance of a vector space over R
two further enrich this vector space, we may define
the following inner product of f , g âˆˆ L
2
âŸ¨f , gâŸ© =
Z
f(x) g(x) dx
this inner product naturally leads to a norm of f âˆˆ L
2
âˆ¥ f âˆ¥ =
p
âŸ¨f , fâŸ©
37 / 89
since f , g âˆˆ L
2 are square integrable, we know
that their inner product exists, because by the
Cauchy-Schwartz inequality




Z
f(x) g(x) dx




2
â©½
Z


f(x)


2
dx Â·
Z

 g(x)


2
dx
we have

âŸ¨f , gâŸ©

 < âˆ
38 / 89
note
â‡’

L
2
, +, Â·,âŸ¨,âŸ©

is a complete inner product space over R
â‡”

L
2
, +, Â·,âŸ¨,âŸ©

is an instance of a Hilbert space H over R
39 / 89
Hilbert spaces are a dime a dozen . . .
they are complete inner product spaces of possibly infinite dimensionality
a vector space V is complete, if every Cauchy sequence in V converges in V
for example, the fields R and Q are both vector spaces over themselves; yet,
while R is complete, Q is not because, say, the Cauchy sequence
2, 2.7, 2.71, 2.718, 2.7182, . . .
of elements of Q converges to e which is not an element of Q
indeed, completeness might be best understood by means of counter examples . . .
40 / 89
(counter) example
consider the set of continuously differentiable functions over R for
which | f (x)| â©½ 1 for all x âˆˆ R
together with an â€œappropriately definedâ€ inner product, functions in
this set form an inner product space I
the family of functions f (x) = tanh(Î²x), Î² > 0 lives in this space
given a growing sequence Î²n = n of parameters, the sequence
fn(x) = tanh(Î²nx)
is a Cauchy sequence of functions in I that converges to sign(x)
but sign(x) is not continuously differentiable â‡’ I is not complete
â‡’ I is not a Hilbert space
41 / 89
Hilbert space
inner product
completeness
Banach space
norm
completeness
inner product space
inner product
normed space
norm
(triangle inequality)
metric space
distance
topological space
points
neighbourhoods
vector space
axioms w.r.t. +, Â·
manifold
is a
is a
is a
is a
is a is a
is a
is a
is a
42 / 89
note
to be on the safe side when working with Hilbert spaces, we still need
more structure, namely the idea of a reproducing kernel Hilbert space
(RKHS)
now, down the rabbit hole . . .
43 / 89
disclaimer
we distinguish between a function for which we
write f and its value at x for which we write f(x)
in other words
f âˆˆ H is a vector
f(x) âˆˆ R is an entry of f
for clarity (trust us ;-), we henceforth write
f(Â·) âˆˆ H
f(x) âˆˆ R
44 / 89
evaluation functional
let H be a Hilbert space of functions f : R
m â†’ R
the linear operator
Î´x : H â†’ R
such that
Î´x

f(Â·)

= f(x)
is called the (Dirac) evaluation functional at x
45 / 89
reproducing kernel Hilbert space
a Hilbert space H of functions f : R
m â†’ R is said to be a reproducing
kernel Hilbert space (RKHS) if, for all x âˆˆ R, the evaluation functional
Î´x is continuous at any f(Â·) âˆˆ H
46 / 89
observe
now we know what an RKHS is, but . . .
questions
what does this definition even mean ?
and what about kernel functions ?
47 / 89
observe
now we know what an RKHS is, but . . .
questions
what does this definition even mean ?
and what about kernel functions ?
answer
apparently, we need to dig even deeper . . .
47 / 89
linear operator
let H1, H2 be Hilbert spaces (of functions) over R
a function O : H1 â†’ H2 is a linear operator, if, for
any f(Â·), g(Â·) âˆˆ H1 and Î± âˆˆ R
O

Î± f(Â·)

= Î± O

f(Â·)

O

f(Â·) + g(Â·)

= O

f(Â·)

+ O

g(Â·)

if H2 = R, then O is called a linear functional on H1
48 / 89
examples
a matrix M âˆˆ R
mÃ—n
is a linear operator that maps from the Hilbert space R
n
to the
Hilbert space R
m s.t. for Euclidean vectors f, g and a scalar Î±
M

Î± f

= Î± M f
M

f + g

= M f + M g
the differentiation operator d
dx maps from the Hilbert space of functions
R â†’ R

to
the Hilbert space of functions
R â†’ R

s.t. for functions f(Â·), g(Â·) and a scalar Î±
d
dx

Î± f(Â·)

= Î±
d
dx
f(Â·)
d
dx

f(Â·) + g(Â·)

=
d
dx
f(Â·) + d
dx
g(Â·)
49 / 89
disclaimer
in what follows, we write
âŸ¨Â·, Â·âŸ©
to denote an appropriate inner product for a generic Hilbert space H
â‡” âŸ¨Â·, Â·âŸ© might be an inner product other than the one considered above
50 / 89
note
an inner product âŸ¨Â·, Â·âŸ© maps from H Ã— H to R
if we fix some h(Â·) âˆˆ H, then


Â·, h(Â·)

is a linear functional which maps from H to R
indeed, for any f(Â·), g(Â·) âˆˆ H and Î± âˆˆ R, we have


Î± f(Â·), h(Â·)

= Î±


f(Â·), h(Â·)



f(Â·) + g(Â·), h(Â·)

=


f(Â·), h(Â·)

+


g(Â·), h(Â·)

51 / 89
Riesz representation theorem
in a Hilbert space H, all continuous linear functionals
are of the form âŸ¨Â·, h(Â·)âŸ© for some h(Â·) âˆˆ H
â‡’ since Î´x is a continuous linear function, we must have
Î´x =


Â·, h(Â·)

for some appropriate h(Â·)
52 / 89
kernel function
let S be any non-empty set, then a function
k : S Ã— S â†’ R
is a kernel function, if there exists a Hilbert
space H and a mapping
Ï† : S â†’ H
such that for any si
,sj âˆˆ S
k

si
,sj

=


Ï†

si

, Ï†

sj

53 / 89
kernel function (in plainer English)
let k be a function of any two things x and y of the same nature
x and y may be numbers, real valued vectors, strings, bananas, . . .
if we can transform x and y into (some kind of) vectors x = Ï†(x) and y = Ï†(y) that live in
(some kind of) a Hilbert space H, then there will be (some kind of) an inner product âŸ¨ x, y âŸ©
between x and y
if the value of the inner product âŸ¨ x, y âŸ© equals the value of the function evaluation k(x, y) for
any choice of x and y, then k is a kernel function
â‡” a kernel is a function whose value corresponds to some inner product in some Hilbert space
54 / 89
example
instantiating
S = R
m
si = xi
sj = xj
â€œone can showâ€ that the Gaussian
Ï•

xi
, xj

= e
âˆ’Î² âˆ¥xiâˆ’xjâˆ¥
2
is a kernel â‡” âˆƒ H, Ï† : e
âˆ’Î² âˆ¥xiâˆ’xjâˆ¥
2
=


Ï†

xi

, Ï†

xj

55 / 89
note
in machine learning, the map or transformation Ï† : S â†’ H
is commonly referred to as the feature map and the space
H as the feature space
56 / 89
for a given kernel k, there may be several different feature maps Ï†i
for example, let S = R and k(x, y) = xy
defining two feature maps Ï†1 and Ï†1 as
Ï†1(x) = x and Ï†2(x) = "
âˆšx
2
âˆšx
2
#
we have H1 = R and H2 = R
2 as well as
k

x, y

=


Ï†1(x), Ï†1(y)

H1
= xy
k

x, y

=


Ï†2(x), Ï†2(y)

H2
=
xy
2
+
xy
2
= xy
57 / 89
reproducing kernel
let H be a Hilbert space of functions f : S â†’ R
a kernel k : S Ã— S â†’ R is a reproducing kernel of H, if for all s âˆˆ S and all f(Â·) âˆˆ H
k

Â·,s

âˆˆ H
and


f

Â·

, k

Â·,s
 = f

s

the latter is known as the reproducing property and k

Â·,s

is sometimes said to
be the representer of evaluation at s
58 / 89
claims (without proof)
a reproducing kernel is symmetric and positive definite
H is a RKHS, if and only if it has a reproducing kernel
if H has a reproducing kernel, that reproducing kernel is unique
â‡’ given a reproducing kernel, we can create / design an RKHS H
59 / 89
example
the Gaussian
Ï•

x, y

= e
âˆ’Î² âˆ¥xâˆ’yâˆ¥
2
is symmetric and positive definite . . .
so letâ€™s look at how to construct a corresponding RKHS
60 / 89
let n âˆˆ N and consider the Hilbert space H of functions of the form
f

Â·

=
Xn
i=1
Î±i Ï•

Â·, ai

the value of f âˆˆ H at x is
f

x

=
Xn
i=1
Î±i Ï•

x, ai

61 / 89
note that Ï•

Â·, aj

is itself a function in this space H because we can write it as
Ï•

Â·, aj

=
Xn
i=1
Î³i Ï•

Â·, ai

where
Î³i =

1 if i = j
0 otherwise
â‡’ the value of Ï•

Â·, aj

âˆˆ H at x is Ï•

x, aj

62 / 89
given two elements
f

Â·

=
Xn
i=1
Î±i Ï•

Â·, ai

g

Â·

=
Xn
j=1
Î²j Ï•

Â·, bj

of H, we now define their inner product


f(Â·), g(Â·)

=
Xn
i=1
Xn
j=1
Î±i Î²j Ï•

ai
, bj

63 / 89
we then curiously observe that


f(Â·), g(Â·)

=
Xn
i=1
Î±i
Xn
j=1
Î²j Ï•

ai
, bj

=
Xn
i=1
Î±i g

ai

or, since the kernel is symmetric


f(Â·), g(Â·)

=
Xn
j=1
Î²j
Xn
i=1
Î±i Ï•

bj
, ai

=
Xn
j=1
Î²j
f

bj

64 / 89
more importantly, we observe the reproducing property


f(Â·), Ï•(Â·, x)

=
Xn
j=1
Î±j
Xn
i=1
Î³i Ï•

aj
, x

=
Xn
j=1
Î±j
Xn
i=1
Î³i Ï•

x, aj

=
Xn
j=1
Î±j Ï•

x, aj

= f

x

65 / 89
a representer theorem (without proof)
while we are at it, we also mention the following representer theorem
let S be some set, H be an RKHS of functions over S, and 
xj
, yj
	n
i=1
be a subset of
S, R

, then the solution to
Ë†f(Â·) = argmin
f (Â·)âˆˆH
âˆ¥ f(Â·)âˆ¥ +
Xn
j=1

f

xj

âˆ’ yj
2
can be written as a finite linear combination
Ë†f(Â·) = Xn
j=1
wj k

Â·, xj

where k : S Ã— S â†’ R is a psd kernel and wj âˆˆ R
66 / 89
â‡” although the problem
Ë†f(Â·) = argmin
f (Â·)âˆˆH
âˆ¥ f(Â·)âˆ¥ +
Xn
j=1

f

xj

âˆ’ yj
2
optimize over a (likely) infinite dimensional space H of functions,
its solutions resides in the n-dimensional space spanned by the
functions k

Â·, xj

this explains why kernel LSQ regression works so well
but what we are really interested in is the following . . .
67 / 89
68 / 89
take home message (1)
our Gaussian RBF density models
p

x

=
1
n
Xn
j=1
Ï•Ïƒ

x, xj

are but special cases of the functions
f

x

=
Xn
i=1
Î±i Ï•

x, ai

69 / 89
take home message (2)
â‡’ our Gaussian RBF density models live in some RKHS H
we therefore know 2 things . . .
70 / 89
take home message (3)
1) as they live in an RKHS, they are fairly well behaved
2) as they live in an RKHS, there must be a feature map
Ï† : R
m â†’ H
such that
p

x

=
1
n
Xn
j=1
Ï•Ïƒ

x, xj

=
1
n
Xn
j=1


Ï†

x

, Ï†

xj
 =
*
Ï†

x

,
1
n
Xn
j=1
Ï†

xj

+
71 / 89
take home message (4)
â‡’ an RBF density model is an inner product
p

x

=


Ï†(x), Ï†Â¯

where
Ï†Â¯ =
1
n
Xn
j=1
Ï†

xj

is the feature space sample mean of the xj
72 / 89
(maximum) mean discrepancy
73 / 89
note
we just uncovered a very deep result
/ 89
note
we just uncovered a very deep result, namely that
given a sample X =

x1, x2, . . . , xn
	
of data points,
their Parzen density estimate
p

x

=
1
n
Xn
j=1
Ï•Ïƒ

x, xj

can also be expressed as
p

x

=


Ï†(x), Ï†Â¯ X

74 / 89
note
â‡’ the density estimate p(x) is determined by the vector
Ï†Â¯ X =
1
n
Xn
j=1
Ï†

xj

75 / 89
mean discrepancy
â‡” dealing with two samples X and Y, we can compare
their distributions using the mean discrepancy
MD2

X, Y

=

Ï†Â¯ X âˆ’ Ï†Â¯ Y


2
but . . .
76 / 89
question
we (usually) neither know the feature map Ï† : R
m â†’ H nor the Hilbert space H,
we only know that there have to exist such a map and such a space . . .
but this is to say that we (usually) cannot actually compute the two Hilbert space
vectors Ï†Â¯ X and Ï†Â¯ Y . . .
so how can we then possibly compute MD2 ?
77 / 89
question
we (usually) neither know the feature map Ï† : R
m â†’ H nor the Hilbert space H,
we only know that there have to exist such a map and such a space . . .
but this is to say that we (usually) cannot actually compute the two Hilbert space
vectors Ï†Â¯ X and Ï†Â¯ Y . . .
so how can we then possibly compute MD2 ?
answer
easy . . .
77 / 89
we have Ï†Â¯ X
âˆ’
Ï†Â¯
Y

2
=

1n
Xni=1
Ï†

x
i

âˆ’
1m
Xmj=1
Ï†

yj


2
=
1n2
Xni=1
Xn k=1


Ï†

x
i
, Ï†

x
k

âˆ’
2
nm
Xni=1
Xmj=1


Ï†
x
i
, Ï†

yj

+
1m
2
Xmj=1
Xml=1


Ï†

yj
, Ï†

y
l

78 / 89
and therefore

Ï†Â¯ X âˆ’ Ï†Â¯ Y


2
=
1
n
2
Xn
i=1
Xn
k=1
Ï•

xi
, xk

âˆ’
2
nm
Xn
i=1
Xm
j=1
Ï•

xi
, yj

+
1
m2
Xm
j=1
Xm
l=1
Ï•

yj
, yl

79 / 89
take home message
â‡’ the feature space distance or mean discrepancy
MD2

X, Y

=

Ï†Â¯ X âˆ’ Ï†Â¯ Y


2
can be computed in terms of kernel evaluations
another point scored
by the kernel trick ;-)
80 / 89
question
but does this have an application ?
answer
of course it has . . .
81 / 89
MD for vector quantization
82 / 89
idea
say, we are given a data set
X =

x1, . . . , xn
	
âŠ‚ R
m
which we want to represent in terms of a codebook
W =

w1, . . . , wk
	
âŠ‚ R
m
we can formalize the vector quantization problem as
WË† = argmin
W,|W|=k
MD2

X, W

= argmin
W,|W|=k

Ï†Â¯ X âˆ’ Ï†Â¯ W


2
83 / 89
to minimize MD2

X, W

w.r.t. W, we may proceed like this:
to update our current guess for each wu âˆˆ W, we consider
âˆ‚
âˆ‚wu
MD2

X, W

= 0
and solve the resulting expression for wu
84 / 89
we have
MD2 =
1
n
2
Xn
j=1
Xn
l=1
Ï•

xj
, xl

+
1
k
2
X
k
i=1
X
k
l=1
Ï•

wi
, wl

âˆ’
2
nk
Xn
j=1
X
k
i=1
Ï•

xj
, wi

and thus
âˆ‚
âˆ‚wu
MD2 =
2
k
2
X
k
i=1
Ï•

wi
, wu

Â·

âˆ’Î²

Â·

wi âˆ’ wu

âˆ’
2
n k
Xn
j=1
Ï•

xj
, wu

Â·

âˆ’Î²

Â·

xj âˆ’ wu

=
2 Î²
k
2
X
k
i=1
Ï•

wi
, wu

wu âˆ’
2 Î²
k
2
X
k
i=1
Ï•

wi
, wu

wi
+
2 Î²
n k
Xn
j=1
Ï•

xj
, wu

xj âˆ’
2 Î²
n k
Xn
j=1
Ï•

xj
, wu

wu
85 / 89
equating to 0 and solving for wu yields the following
fixed-point iteration for updating the codebook
w
new
u =
Pn
j=1
xj Ï•

xj
, wu

Pn
j=1 Ï•

xj
, wu

âˆ’
n
k
Pk
i=1 wi Ï•

wi
, wu

Pn
j=1 Ï•

xj
, wu

+
n
k
Pk
i=1 Ï•

wi
, wu

Pn
j=1 Ï•

xj
, wu
 wu
note: this is ITVQ with c = n
k
86 / 89
â‡’ using our definitions from way above, we can update the whole codebook at once
Wnew =
h
X Î¦xw âˆ’
n
k
Â· W Î¦ww +
n
k
Â· W Dwwi
D
âˆ’1
xw
=
h
X Î¦xw +
n
k
Â· W

Dww âˆ’ Î¦ww
i
D
âˆ’1
xw
87 / 89
summary
88 / 89
we now know about
vector quantization
information theoretic vector quantization
a bit of the theory behind reproducing kernel Hilbert spaces
mean discrepancies and mean discrepancy minimization for VQ
outlook: the latter can be of interest in quantum computing
89 / 89