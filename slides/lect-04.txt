Principles of Machine Learning
Prof. Christian Bauckhage
1 / 76
lecture 04
binary classification, logistic regression classifiers, and autodiff
training a binary classifier is one of the most fundamental machine learning tasks
a fundamental model family for binary classification are linear binary classifiers
for these, there exists a whole zoo of training objectives which typically result in
different kinds of classifiers (which typically produce different results)
in this lecture, we focus on a model class of quite some (theoretical) importance,
namely on logistic regression classifiers
we show how to train them using gradient descent and finally touch upon how to
automatize gradient descent by employing incredibly useful autodiff techniques
2 / 76
outline
(linear) binary classifiers
logistic regression classifiers
computation graphs and autodiff
summary
3 / 76
(linear) binary classifiers
4 / 76
binary classification
given a set of annotated training data
D =


xj
, yj
n
j=1
which consists of pairs of observations
xj âˆˆ R
m
from two classes â„¦1 and â„¦2 together
with associated class labels
yj =

+1 if xj âˆˆ â„¦1
âˆ’1 if xj âˆˆ â„¦2
5 / 76
our goal is to train a binary classifier model
f

x

=

+1 if x âˆˆ â„¦1
âˆ’1 if x âˆˆ â„¦2
so that we can classify new observations x
x
6 / 76
question
how to proceed ?
answer
there actually are numerous model classes suitable for our problem
a popular and simple choice is the class of linear binary classifiers
7 / 76
consider this
we may determine a projection vector w
w
8 / 76
consider this
we may determine a projection vector w
it spans a 1-dimensional subspace of R
m
w
8 / 76
consider this
we may determine a projection vector w
it spans a 1-dimensional subspace of R
m
we may compute inner products x
âŠº
j w to
project the training data into this space
w
8 / 76
consider this
we may determine a projection vector w
it spans a 1-dimensional subspace of R
m
we may compute inner products x
âŠº
j w to
project the training data into this space
we may estimate a threshold or bias b
w and b define an (m âˆ’ 1)-dimensional
hyperplane in R
m
H =


x âˆˆ R
m



x
âŠºw = b

w
b
8 / 76
working with the plane we just found, we
can (confidently ?!?) classify any x âˆˆ R
m
f

x

=

+1 if x
âŠºw â©¾ b
âˆ’1 otherwise
x
9 / 76
we could also be more careful and define f x = ï£±ï£´ï£²ï£´ï£³â„¦1 if xâŠºw > b â„¦2 if xâŠºw < b R if xâŠºw = b
in the olden days (up to the 1990s),
R was
called the rejection class
then people forgot about rejection classes
then people rediscovered their usefulness
this rediscovery was sold as a discovery :(
nowadays, our more careful model is thus
an example of an abstaining classifier x
10 / 76
linear binary classifier
a (hard) linear binary classifier is a
model of the following general form
f

x

=

+1 if x
âŠºw â©¾ b
âˆ’1 if x
âŠºw < b
=

+1 if x
âŠºw âˆ’ b â©¾ 0
âˆ’1 if x
âŠºw âˆ’ b < 0
= sign
x
âŠºw âˆ’ b

x
w
b
11 / 76
notation
the signum function is usually defined as
sign
x

=
ï£±
ï£´ï£²
ï£´ï£³
+1 if x > 0
0 if x = 0
âˆ’1 if x < 0
however, in this course, we shall consider
sign
x

=

+1 if x â©¾ 0
âˆ’1 if x < 0
12 / 76
note
there exist numerous special kinds of linear binary classifiers f

x

= sign
x
âŠºw âˆ’ b

they distinguish themselves in how they estimate weight vector w and bias term b
â‡” they distinguish themselves with regard to the optimization criterion they consider
LDA perceptron learning logistic regression L2 SVM
. . .
13 / 76
principle of machine learning
for most machine learning tasks, there are several models which can solve them
which model to choose depends on the application context and goals we aim for
unfortunately, we often have to trade off goals
accuracy / reliability of model predictions
explainability of of model predictions
low application efforts (compute)
low training efforts (compute)
low training data needs
.
.
.
14 / 76
note
linear binary classifiers can (obviously) only work for linearly separable classes
solutions for the case of non-linearly separable classes will be studied later on
linearly separable classes non-linearly separable classes
15 / 76
note
we once again have
f

x

= sign
x
âŠºw âˆ’ b

â‡”
âˆ’1 x1 x2 Â· Â· Â· xm
h(a)
y
b w1 w2
wm
â‡” a single binary classifier can be understood as a single artificial neuron
16 / 76
disclaimer
(the many flavors of) support vector machines provide the arguably most reliable
or most robust linear binary classifiers
however, for the remainder of todayâ€™s lecture, we will focus on logistic regression
classifiers, because
in contrast to SVMs, we can easily study them based on what we already know
if we understand them, we have a solid basis for understanding neural networks
finally, there is a probabilistic interpretation of logistic regression which will be of
interest later on
17 / 76
logistic regression classifiers
18 / 76
preparatory remarks (1)
the main problem of training a binary classifier
f

x

= sign
x
âŠºw âˆ’ b

from data D is to estimate optimal parameters
wË†, Ë†b = argmin
w,b
L

w, b

 D

the choice of the loss function L is up to us and we already saw that different choices
may result in different flavors of classifiers
depending on which flavor we opt for, optimal weights wË† and bias Ë†b can be estimated
consecutively (LDA first determines wË† and then Ë†b) or simultaneously (as weâ€™ll do next)
19 / 76
preparatory remarks (2)
looking at the expression x
âŠºw âˆ’ b, we see another inner product in disguise, namely
x
âŠºw âˆ’ b = Ï†
âŠº
xÏ‰
with these R
m+1 vectors
Ï†x =

âˆ’1
x

Ï‰ =

b
w

we have already seen that such feature representations may have advantages . . .
however, in what follows, we will deliberately not work with this representation
20 / 76
preparatory remarks (3)
in what follows, we assume training data
D =


xj
, yj
n
j=1
with data points xj âˆˆ R
m and class labels
yj âˆˆ

âˆ’1, +1
	
we insist on bipolar labels yj âˆˆ {âˆ’1, +1} rather than on binary labels yj âˆˆ {0, 1}
while the latter have their use, the former are essential to the mathematical ideas
(â‡” specific loss function design) which we will study next . . .
21 / 76
question
OK, now for real, how do we estimate optimal parameters of a binary classifier
f

x

= sign
x
âŠºw âˆ’ b

?
answer
well, one approach for logistic regression classifiers involves gradient descent
qu / 76
question
OK, now for real, how do we estimate optimal parameters of a binary classifier
f

x

= sign
x
âŠºw âˆ’ b

?
answer
well, one approach for logistic regression classifiers involves gradient descent
question
but isnâ€™t sign discontinuous and has vanishing gradients almost everywhere ?
22 / 76
answer
yes, but the following surrogate function is continuous and continuously differentiable
tanh
Î² x

=
sinh(Î² x)
cosh(Î² x)
=
e
Î²x âˆ’ e
âˆ’Î²x
e
Î²x + eâˆ’Î²x
=
1 âˆ’ e
âˆ’2Î²x
1 + eâˆ’2Î²x
=
2
1 + eâˆ’2Î²x
âˆ’ 1
it maps any x âˆˆ R into the open interval (âˆ’1, +1) and also has the following property
lim
Î²â†’âˆž
tanh
Î² x

= sign
x

23 / 76
illustration
sign
x
âŠºw âˆ’ b

b
âˆ’1
+1
x
|w
24 / 76
illustration
tanh
Î²

x
âŠºw âˆ’ b

b
âˆ’1
+1
x
|w
Î² = 0.5
Î² = 1.0
Î² = 2.0
Î² = 4.0
Î² = 8.0
25 / 76
principle of machine learning
it can be a good idea to relax logically reasonable models like
f

x

= sign
x
âŠºw âˆ’ b

to computationally (much) more manageable models such as
f

x

= tanh
Î²

x
âŠºw âˆ’ b


26 / 76
towards the logistic loss . . .
now, if we agree to train a binary classifier of the form
f

x

= tanh
Î²

x
âŠºw âˆ’ b


our goal is to adjust its parameters such that we have
the following behavior on our training data
âˆ€ j : f

xj

= tanh
Î²

x
âŠº
j w âˆ’ b


â‰ˆ yj
it thus seems tempting to minimize the quadratic loss
L

w, b, Î²


f , D

=
1
n
Xn
j=1

tanh
Î²

x
âŠº
j w âˆ’ b


âˆ’ yj
2
but let us widen our horizon . . .
27 / 76
observe
the hyperbolic tangent is an odd function such that
âˆ’ tanh
x

= tanh
âˆ’x

this has interesting implications for our training goal
tanh
Î²

x
âŠº
j w âˆ’ b


â‰ˆ yj
28 / 76
note
since the hyperbolic tangent is odd and we insist on
yj âˆˆ

Â±1
	
â‡” y
2
j = 1
we have this remarkable sequence of equivalences
tanh
Î²

x
âŠº
j w âˆ’ b


â‰ˆ yj
â‡” yj tanh
Î²

x
âŠº
j w âˆ’ b


â‰ˆ 1
â‡” tanh
yj Î²

x
âŠº
j w âˆ’ b


â‰ˆ 1
29 / 76
note
since we also have tanh
Î² x

=
2
1+eâˆ’2Î²x âˆ’ 1,
we can continue the above steps as follows
tanh
yj Î²

x
âŠº
j w âˆ’ b


â‰ˆ 1
â‡”
2
1 + e
âˆ’yj2Î²(x
âŠº
j wâˆ’b)
â‰ˆ 2 (next, divide by 2 and take reciprocal)
â‡” 1 + e
âˆ’yj2Î²(x
âŠº
j wâˆ’b) â‰ˆ 1
â‡” log
1 + e
âˆ’yj2Î²(x
âŠº
j wâˆ’b)

â‰ˆ 0
30 / 76
logistic loss
â‡’ we may consider the (average) logistic loss
L

w, b, Î²

 D

=
1
n
Xn
j=1
log
1 + e
âˆ’2Î²yj(x
âŠº
j wâˆ’b)

as a minimization objective for classifier training
â€œone can showâ€ that this function is always convex
â‡” the logistic loss has unique global minimizers wË†, Ë†b, Î²Ë†
31 / 76
remarks
L as defined above is called a logistic loss because
it involves functions of the form
Ïƒ

Î³ x

=
1
1 + eâˆ’Î³x
also known as logistic functions
weâ€™ll say more about these later on
32 / 76
gradients of the logistic loss
if we decide for gradient descent to determine
wË†, Ë†b, Î²Ë† = argmin
w,b,Î²
L

w, b, Î²

= argmin
w,b,Î²
1
n
Xn
j=1
log
1 + e
âˆ’2Î²yj(x
âŠº
j wâˆ’b)

we must be able to compute the partial derivatives
âˆ‚L
âˆ‚Î²,
âˆ‚L
âˆ‚b
, and âˆ‚L
âˆ‚w
we therefore note . . .
33 / 76
we have Lw, b, Î²

=
1n
Xnj=1
L
j

w, b, Î²

and therefore âˆ‚L
âˆ‚Î²
=
1n
Xnj=1
âˆ‚
L
j
âˆ‚Î²
âˆ‚
Lâˆ‚b
=
1n
Xnj=1
âˆ‚
L
j
âˆ‚
b
âˆ‚
L
âˆ‚
w
=
1n
Xnj=1
âˆ‚
L
j
âˆ‚
w
34 / 76
we further have
Lj

w, b, Î²

= log
1 + e
âˆ’2Î²yj(x
âŠº
j wâˆ’b)

= log
gj

hj

w, b, Î²

which involves
gj

hj

= 1 + e
hj
hj

w, b, Î²

= âˆ’2 Î² yj

x
âŠº
j w âˆ’ b

35 / 76
â‡’ we have
âˆ‚Lj
âˆ‚Î² =
âˆ‚Lj
âˆ‚gj
âˆ‚gj
âˆ‚hj
âˆ‚hj
âˆ‚Î² =
1
gj
Â· e
hj
Â·

âˆ’2 yj

x
âŠº
j w âˆ’ b

âˆ‚Lj
âˆ‚b
=
âˆ‚Lj
âˆ‚gj
âˆ‚gj
âˆ‚hj
âˆ‚hj
âˆ‚b
=
1
gj
Â· e
hj
Â·

2 Î² yj

âˆ‚Lj
âˆ‚w
=
âˆ‚Lj
âˆ‚gj
âˆ‚gj
âˆ‚hj
âˆ‚hj
âˆ‚w
=
1
gj
Â· e
hj
Â·

âˆ’2 Î² yj xj

36 / 76
note
if you feel rusty about your symbolic differentiation skills, then simply fire up sympy
from sympy import symbols, diff, log, exp
x, y, w, b, beta = symbols(â€™x y w b betaâ€™, real=True)
h = -2 * y * beta * (x*w - b)
g = 1 + exp(h)
L = log(g)
print (diff(L, beta))
print (diff(L, b))
print (diff(L, w))
â‡” for decades now, computers can perform symbolic differentiation for us (without AI)
37 / 76
â‡’ with all this in place, we can run
guess
w
0, b
0, Î²
0
for
k
=
0, 1, 2, . . .
Î²
k
+
1
=
Î²
k
âˆ’
Î·
Â·
1n
Xnj=1
âˆ‚
L
j
âˆ‚Î²
k
b
k
+
1
=
b
k
âˆ’
Î·
Â·
1n
Xnj=1
âˆ‚
L
j
âˆ‚
b
k
w
k
+
1
=
w
k
âˆ’
Î·
Â·
1n
Xnj=1
âˆ‚
L
j
âˆ‚
w
k
so letâ€™s do it . . .
38 / 76
numpy
import numpy as np
# training
matXtrn = ...
vecYtrn = ...
params = init_parameters(matXtrn.shape[0])
params = train_LR_model(params, matXtrn, vecYtrn)
# testing
matXtst = ...
vecYtst = apply_LR_model(matXtst, params)
where . . .
39 / 76
def init_parameters(data_dim):
wgts = 0.5 * np.ones(data_dim)
bias = 0.0
beta = 1.0
return wgts, bias, beta
def apply_LR_model(X, params):
w, b, B = params
return np.tanh(B * (X.T @ w - b))
so far so good, but here we go . . .
40 / 76
def train_LR_model(params, X, y, eta=0.05, kmax=100):
w, b, B = params
for k in range(kmax):
h = -2 * B * y * (X.T @ w - b)
g = 1 + np.exp(h)
dLdg = 1 / g
dgdh = np.exp(h)
dhdB = -2 * y * (X.T @ w - b)
dhdb = 2 * y * B
dhdw = -2 * y * B * X
dLdB = np.mean(dLdg * dgdh * dhdB)
dLdb = np.mean(dLdg * dgdh * dhdb)
dLdw = np.mean(dLdg * dgdh * dhdw, axis=1)
B = B - eta * dLdB
b = b - eta * dLdb
w = w - eta * dLdw
return w, b, B
41 / 76
results
soft classifier hard classifier
f

x

= tanh
Î²

x
âŠºw âˆ’ b

f

x

= sign
tanh
Î²

x
âŠºw âˆ’ b

42 / 76
observe
apparently, the math checks out !!!
â‡” our tedious numpy implementation yields results we would hope for
however, let us dwell on the â€œtediousâ€ part in the previous statement
in the early 2010s, the machine learning landscape changed drastically
this was largely due to the availability of big data for training and (minor)
theoretical progress in deep learning (DL has a longer history than most
people think, i.e. it had already been around for quite some time)
but what was also crucially important was that machine learners (finally)
adopted the use of automatic differentiation and open-sourced new tools
nowadays, there are several python autodiff libraries and an interesting
one of those is jax . . .
43 / 76
jax
import jax
import jax.numpy as jnp
# training
matXtrn = ...
vecYtrn = ...
params = init_parameters(matXtrn.shape[0])
params = train_LR_model(params, matXtrn, vecYtrn)
# testing
matXtst = ...
vecYtst = apply_LR_model(matXtst, params)
where . . .
44 / 76
def init_parameters(data_dim):
wgts = 0.5 * jnp.ones(data_dim)
bias = 0.0
beta = 1.0
return wgts, bias, beta
def apply_LR_model(X, params):
w, b, B = params
return jnp.tanh(B * (X.T @ w - b))
so far, the only difference is our use of jnp, but now . . .
45 / 76
def loss_fnct(params, X, y):
w, b, B = params
return jnp.mean(jnp.log(1 + jnp.exp(-2 * B * y * (X.T @ w - b))))
@jax.jit
def update_parameters(params, X, y, eta):
grads = jax.grad(loss_fnct)(params, X, y)
return [param - eta * grad for param, grad in zip(params, grads)]
def train_LR_model(params, X, y, eta=0.05, kmax=100):
for k in range(kmax):
params = update_parameters(params, X, y, eta)
what ???
46 / 76
note
yes, this is it !!!
all we need to do is to define (a properly vectorized) loss function and then let
the jax.grad autodiff capabilities worry about all the (error prone) details
our code could be more efficient (â‡” avoid for loops) but we aim at readability
also, using optax, we could â€œeasilyâ€ invoke ADAM or other specific optimizers
47 / 76
principle of machine learning
work with autodiff libraries (pytorch, tensorflow, jax (!!!), . . . ) wherever possible
letâ€™s touch upon the underlying theory . . .
48 / 76
computation graphs and autodiff
49 / 76
it has become a tradition for ML courses to preface discussions of what autodiff is
with discussions of what it is not, so here we go . . .
autodiff is not automatic symbolic differentiation as in
from sympy import symbols, diff
x, y = symbols(â€™x yâ€™, real=True)
def fnct(x, y):
return x**2 + y**2
def grad(f):
dfdx = diff(f(x, y), x)
dfdy = diff(f(x, y), y)
return (dfdx, dfdy)
print (grad(fnct))
>>> (2*x, 2*y)
50 / 76
â‡’ autodiff is not computation with symbols but with numbers, however . . .
autodiff is also not numerical differentiation as in
def fnct(x: float, y: float) -> float:
return x**2 + y**2
def grad(f, x, y, dx=1e-5, dy=1e-5):
dfdx = (f(x+dx, y) - f(x-dx, y)) / (2*dx)
dfdy = (f(x, y+dy) - f(x, y-dy)) / (2*dy)
return (dfdx, dfdy)
print (grad(fnct, 2, 4))
>>> (4.000000000026205, 7.999999999874773)
51 / 76
autodiff (AD)
in a sense, autodiff (AD) systems combine the best of both the above approaches
they â€œknowâ€ about (symbolic) derivatives of (basic) mathematical functions
(negative, add, mul, . . . , exp, log, sin, . . . ) and about chain rules for
deriving composite univariate and multivariate functions
they use this â€œknowledgeâ€ to differentiate functions whose arguments and
parameters have been instantiated to certain numerical values
â‡” AD applies symbolically informed data structures to numerically evaluate gradients
52 / 76
AD can operate in two fashions called forward mode and reverse mode
next, we will focus on the latter since it is (usually) more important in ML
finally, for much of what follows, we must keep in mind the difference between
symbolic expressions f

x

= x
2 and evaluations f

x


x=4 = f

4

= 16
â‡” although we will use symbols (X, y, w, b, Î²) to refer to arguments and parameters of a function (L), we will
largely assume that these have been instantiated to specific numerical values (eg. w = [1, 0]
âŠº, b = 2.3, . . .)
so, letâ€™s go for it . . .
53 / 76
to begin with, we set up a more general learning task than in the previous sections
â‡” next, we will consider the problem of minimizing a regularized logistic loss function
L = Ll + Lr
which sums these two terms
Lr = Lr

w

=
1
2
w
âŠºw
Ll = Ll

w, b, Î²

=
1
n
X
j
log
1 + exp
âˆ’2 Î² yj

x
âŠºw âˆ’ b


the what, why, and how of regularization will be studied later on
54 / 76
observe that we may â€œreduceâ€ notational clutter by vectorizing Ll
Ll =
1
n
X
j
log
1 + exp
âˆ’2 Î² yj

x
âŠºw âˆ’ b


â‡” Ll =
1
n
1
âŠº
h
log
1 + exp
âˆ’2 Î² y âŠ™

X
âŠºw âˆ’ 1 b

i
where the functions log and exp now act element-wise on vectors
55 / 76
we next introduce variables
v1 =

X
âŠºw âˆ’ 1 b

v2 = âˆ’2 Î² y âŠ™ v1
v3 = 1 + exp
v2

v4 = log
v3

v5 =
1
n
1
âŠº
v4
v6 =
1
2
w
âŠºw
v7 = v5 + v6
to break down how to compute L for given
values of data X, y and parameters w, b, Î²
into bite sized steps
here is what an autodiff system would do
V1 = X
âŠºw
V2 = V1 âˆ’ 1b
V3 = y âŠ™ V2
V4 = Î² Â· V3
V5 = âˆ’2 Â· V4
V6 = exp(V5)
V7 = 1 + V6
V8 = log(V7)
V9 = 1
n
1
âŠºV8
V10 = w
âŠºw
V11 = 1
2
Â· V10
V12 = V9 + V11
as this is too fine-grained for us to bother
with, we shall stick with our variables . . .
56 / 76
computation graph
â‡’ we can visualize how to compute L = v7 in terms of this computation graph
w
b
Î²
X y
+
v1 v2 v3 v4 v5
v6
v7
57 / 76
remarks
any computation graph G =

V, E

is a directed acyclic graph (DAG)
all edges
u, v

âˆˆ E âŠ† V Ã— V are ordered pairs of vertices (â‡” point from u to v)
no path in G starts and ends at the same vertex v âˆˆ V (â‡” there are no cycles)
also, recall the notions of the two sets
pred
v

=


u âˆˆ V



u, v

âˆˆ E

succ
v

=


w âˆˆ V



v, w

âˆˆ E

of predecessors (parents) and successors (children) of a vertex
58 / 76
forward pass
w
b
Î²
X y +
v1 v2 v3 v4 v5
v6
v7
given our computation graph, we â€œseeâ€
how to iteratively compute v7, namely
for k = 1, . . . , K(= 7)
vk = fk

pred
vk

for instance, for vertex v2, we have the following
pred
v2

=

y, Î², v1
	
f2

pred
v2
 = âˆ’2 Î² y âŠ™ v1
59 / 76
w
b
Î²
X y +
v1 v2 v3 v4 v5
v6
v7
given our computation graph, we â€œseeâ€
how v7 depends on parameters w, b, Î²
for instance
âˆ‚v7
âˆ‚Î² =
âˆ‚v7
âˆ‚v5
âˆ‚v5
âˆ‚Î² +
âˆ‚v7
âˆ‚v6
âˆ‚v6
âˆ‚Î² =
âˆ‚v7
âˆ‚v5
âˆ‚v5
âˆ‚v4
âˆ‚v4
âˆ‚v3
âˆ‚v3
âˆ‚v2
âˆ‚v2
âˆ‚Î² + 0
âˆ‚v7
âˆ‚w
=
âˆ‚v7
âˆ‚v5
âˆ‚v5
âˆ‚w
+
âˆ‚v7
âˆ‚v6
âˆ‚v6
âˆ‚w
=
âˆ‚v7
âˆ‚v5
âˆ‚v5
âˆ‚v4
âˆ‚v4
âˆ‚v3
âˆ‚v3
âˆ‚v2
âˆ‚v2
âˆ‚v1
âˆ‚v1
âˆ‚w
+
âˆ‚v7
âˆ‚v6
âˆ‚v6
âˆ‚w
but letâ€™s be clever . . .
60 / 76
consider, say the derivative âˆ‚v7 âˆ‚v1 = âˆ‚v7 âˆ‚v5 âˆ‚v5 âˆ‚v4 âˆ‚v4 âˆ‚v3 âˆ‚v3 âˆ‚v2 âˆ‚
v
2
âˆ‚
v
1
and observe these identities âˆ‚v7 âˆ‚v5 âˆ‚v5 âˆ‚v4 âˆ‚v4 âˆ‚v3 âˆ‚v3 âˆ‚v2 âˆ‚v2 âˆ‚v1 = âˆ‚
v
2
âˆ‚
v
1
âˆ‚
v
3
âˆ‚
v
2
âˆ‚
v
4
âˆ‚
v
3
âˆ‚
v
5
âˆ‚
v
4
âˆ‚
v
7
âˆ‚
v
5
=
âˆ‚
v
2
âˆ‚
v
1
âˆ‚
v
3
âˆ‚
v
2
âˆ‚
v
4
âˆ‚
v
3
âˆ‚
v
7
âˆ‚
v
4
=
âˆ‚
v
2
âˆ‚
v
1
âˆ‚
v
3
âˆ‚
v
2
âˆ‚
v
7
âˆ‚
v
3
=
âˆ‚
v
2
âˆ‚
v
1
âˆ‚
v
7
âˆ‚
v
2
61 / 76
note
we henceforth write
Â¯vk â‰¡
âˆ‚vK
âˆ‚vk
and observe that
we particularly have
Â¯vK =
âˆ‚vK
âˆ‚vK
= 1
we generally have
Â¯vk =
X
vl âˆˆ succ(vk)
âˆ‚vl
âˆ‚vk
Â¯vl
for instance, we need summation to compute
wÂ¯ =
âˆ‚v1
âˆ‚w
Â¯v1 +
âˆ‚v6
âˆ‚w
Â¯v6
62 / 76
backward pass
w
b
Î²
X y +
v1 v2 v3 v4 v5
v6
v7
after a forward pass, we can iteratively
compute instantiated gradients, namely
for k = K âˆ’ 1, . . . , 1
Â¯vk =
P
vl âˆˆ succ(vk)
âˆ‚vl
âˆ‚vk
Â¯vl
63 / 76
â‡’ given instances of X, y, w, b, Î², we can compute
variable values
v1 =

X
âŠºw âˆ’ 1 b

v2 = âˆ’2 Î² y âŠ™ v1
v3 = 1 + exp
v2

v4 = log
v3

v5 =
1
n
1
âŠº
v4
v6 =
1
2
w
âŠºw
v7 = v5 + v6
gradient values (â‡” adjoints)
Â¯v7 = 1
Â¯v6 = v6
Â¯v5 = v5
Â¯v4 =
1
n
1Â¯v5
Â¯v3 =
1
v3
âŠ™ Â¯v4
Â¯v2 = exp(v2) âŠ™ Â¯v3
Â¯v1 = âˆ’2 Î² y âŠ™ Â¯v2
64 / 76
â‡’ given values for X, y, w, b, Î²,
we can compute these values
Î²Â¯ = âˆ’2

y âŠ™ v1
âŠº
Â¯v2
Â¯b = âˆ’1
âŠº
Â¯v1
wÂ¯ = XÂ¯v1 + w Â· Â¯v6
â‡’ we can use these values to perform
gradient descent
guess w0, b0, Î²0
for k = 0, 1, 2, . . .
Î²k+1 = Î²k âˆ’ Î· Â· Î²Â¯
k
bk+1 = bk âˆ’ Î· Â· Â¯bk
wk+1 = wk âˆ’ Î· Â· wÂ¯k
try it !!!
65 / 76
question
OK, but we did all of this manually . . . where is the autodiff part ?
answer
here . . .
66 / 76
AD systems build primal graphs
w
b
Î²
X y +
v1 v2 v3 v4 v5
v6
v7
and their adjoints
wÂ¯
Â¯b
Î²Â¯
XÂ¯ yÂ¯
vÂ¯1 vÂ¯2 vÂ¯3 vÂ¯4 vÂ¯5
vÂ¯6
vÂ¯7
under the hood, they use smarter data structures
67 / 76
most crucially, AD systems completely revamp the underlying â€œmath engineâ€
â‡” they redefine all math operations negative, add, mul, . . . , exp, log, sin, . . .
on the next slides, we didactically demonstrate this but only consider add and mul
â‡” we define a class which we can use to declare numerical variables and to add and
multiply such variables . . .
68 / 76
variables (â‡” instances of arguments, parameters, constants) are containers with a
primal entry, an adjoint entry, and a successor list
class Var:
def __init__(self, value):
self.prml = value
self.adjn = 1.0
self.succ = []
def __add__(self, other):
res = Var(self.prml + other.prml)
# dres/dself = 1
# dres/dother = 1
self.succ.append(( 1, res))
other.succ.append((1, res))
return res
def __mul__(self, other):
res = Var(self.prml * other.prml)
# dres/dself = other.prml
# dres/dother = self.prml
self.succ.append((other.prml, res))
other.succ.append((self.prml, res))
return res
69 / 76
with this we can consider computation graphs of simple expressions and represent
them as (manually) sorted lists, for example
x = Var(2)
y = Var(4)
a = x * y
b = a + x
comp_graph = [x, y, a, b]
of course, given function definitions such as
def f(x, y):
return x * y + x
a real autodiff system would build such a graph (and its representation) automatically
however, our didactic demo is not that fancy but really mainly aims at the following . . .
70 / 76
given a list such as comp graph, we can apply this backward pass gradient function
def grad(graph):
for vk in graph[-2::-1]:
vk.adjn = sum( dvldvk * vl.adjn for dvldvk, vl in vk.succ )
indeed, when we run
grad(comp_graph)
print (fâ€™b_bar = {b.adjn}â€™)
print (fâ€™a_bar = {a.adjn}â€™)
print (fâ€™x_bar = {x.adjn}â€™)
print (fâ€™y_bar = {y.adjn}â€™)
we obtain this result
>>> b_bar = 1.0
>>> a_bar = 1.0
>>> x_bar = 5.0
>>> y_bar = 2.0
71 / 76
this makes sense, since the symbolic expression
b = a + x = xy + x
has the following four symbolic partial derivatives
âˆ‚b
âˆ‚b = 1
âˆ‚b
âˆ‚a= 1
âˆ‚b
âˆ‚x = y + 1
âˆ‚b
âˆ‚y = x
which, when evaluated for x = 2 and y = 4 become
Â¯b = 1 Â¯a = 1
Â¯x = 5 Â¯y = 2
72 / 76
concluding remarks
the ideas behind (forward- and reverse mode) autodiff techniques date back to the
1960s and 1970s [1, 2]
for whatever reasons, it really took until the late 2000s until they were adopted for
mainstream machine learning
once this happened though, it helped kicking off the deep learning revolution since
autodiff seamlessly simplifies error-backpropagation [3] for neural network training
in this course, we advocate using jax rather than, say, pytorch or tensorflow
while it may not be gentle to beginners, it is more general or widely applicable than
other libraries and is now also increasingly adopted by key industrial players . . .
73 / 76
summary
74 / 76
we now know about
the binary classification problem
linear binary classifiers in general and logistic regression classifiers in particular
the practically ridiculously important technique of automatic differentiation (autodiff)
75 / 76
references
[1] R.E. Wengert. A Simple Automatic Derivative Evaluation Program. Communications of
the ACM, 7(8), 1964.
[2] S. Linnainmaa. Taylor Expansion of the Accumulated Rounding Error. BIT Numerical
Mathematics, 16(2), 1976.
[3] D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning Representations by
Back-propagating Errors. Nature, 323(6088), 1986.
76 / 76