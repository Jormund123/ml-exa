Principles of Machine Learning
Prof. Christian Bauckhage
1 / 85
lecture 11
support vector machines (part 2)
training a binary classifier is one of the most fundamental machine learning problems
an important model family for binary classifiers is the family of linear binary classifiers
we continue studying the most powerful or robust variant of linear binary classifiers,
i.e. SVMs and we shall see, that there are actually different kinds of SVMs, some of
which are less well known but very easy to work with . . .
in several previous lectures, we emphasized the occurrence of Gram matrices in
the methods we studied and also came across the notion of kernel functions . . .
in this lecture, we will see how both are connected and why both are interesting
to be specific, we will study the powerful idea of the kernel trick and will see that
it allows us to use linear models to deal with non-linear problems
2 / 85
outline
recap
working with â€œbetterâ€ SVM loss functions
least squares SVMs
L2 SVMs
limitations of linear models
the kernel trick
kernel engineering
kernel SVMs
summary
3 / 85
recap
4 / 85
trained with labeled training data D = 
xj, yjnj=1 xj âˆˆ â„¦1 âˆª â„¦2	 âŠ‚ Rm yj = +1, if xj âˆˆ â„¦1 âˆ’1, if xj âˆˆ â„¦2 a linear binary classifier is a model y = f(x) = +1 if wâŠºx â©¾ b âˆ’1 if wâŠºx < b = signwâŠºx âˆ’ b
that predicts a class label
y for
x
w
b
x
5 / 85
â‡’ the problem is to learn suitable
w âˆˆ R
m
b âˆˆ R
from the available training data
SVMs do this by estimating w such that
it maximizes the margin Ï between the
(projected) data from both classes
once w is available, b is easy to come by
w
Ï
6 / 85
those training data vectors that define
the margin and hence the separating
hyperplane are called support vectors
7 / 85
principle of machine learning
the max-margin criterion is a good idea, as SVMs do not
require any (implicit) assumptions about how training
data are distributed
this is in contrast to other kinds of linear binary classifiers
(naÂ¨Ä±ve) LSQ, LDA, nearest mean, . . .
alas, the resulting robustness of SVMs comes at the cost
of (slightly) more challenging training procedures
8 / 85
in what follows, we let
X =
ï£®
ï£°
| | |
x1 x2 Â· Â· Â· xn
| | |
ï£¹
ï£» âˆˆ R
mÃ—n
y =

y1 y2 Â· Â· Â· yn
âŠº
âˆˆ R
n
1 =

1 1 Â· Â· Â· 1
âŠº
âˆˆ R
n
0 =

0 0 Â· Â· Â· 0
âŠº
âˆˆ R
n
we also introduce
zj = yj xj âˆˆ R
m
Z =
ï£®
ï£°
| | |
z1 z2 Â· Â· Â· zn
| | |
ï£¹
ï£» âˆˆ R
mÃ—n
and a vector of slack variables
Î¾ =

Î¾1 Î¾2 Â· Â· Â· Î¾n
âŠº
âˆˆ R
n
Î¾ â©¾ 0
9 / 85
question
why slack variables ?
answer
often, the two classes â„¦1 and â„¦2 are not linearly separable . . .
linearly separable data non linearly separable data
10 / 85
answer (cntd.)
in this case, there cannot exist a separating hyperplane with
âˆ€ j : yj
Â·

w
âŠº
xj âˆ’ b

â©¾ 1
but, introducing slacks Î¾j â©¾ 0, there exist (many) planes with
âˆ€ j : yj
Â·

w
âŠº
xj âˆ’ b

â©¾ 1 âˆ’ Î¾j
â‡” w.r.t. to such planes, a misclassified point xj
is one where Î¾j > 1
since we want to reduce misclassifications,
a reasonable idea is trying to solve the . . .
11 / 85
primal problem of L1 SVM training
argmin
w, b,Î¾
1
2
w
âŠºw + C 1
âŠºÎ¾
s.t. Z
âŠºw âˆ’ b y âˆ’ 1 + Î¾ â©¾ 0
Î¾ â©¾ 0
parameter C > 0 allows for â€œtuning the slackâ€
12 / 85
dual problem of L1 SVM training
argmin
Âµ
1
2
Âµ
âŠºZ
âŠºZ Âµ âˆ’ 1
âŠºÂµ
s.t.
y
âŠºÂµ = 0
0 â©½ Âµ â©½ C 1
13 / 85
note
because of the box constraint
0 â©½ Âµ â©½ C 1
L1 SVM training is an unwieldy problem (which can of course still be solved)
however, the primal minimization objective
L

w, Î¾

 C

=
1
2
w
âŠºw + C
Xn
j=1
Î¾j =
1
2
w
âŠºw + C 1
âŠºÎ¾
is but one example of a possible a loss function and we may consider others
for instance . . .
14 / 85
working with â€œbetterâ€ SVM loss functions
15 / 85
without much ado . . .
least squares loss
L

w, Î¾

 C

=
1
2
w
âŠºw +
1
2
C
Xn
j=1
Î¾
2
j =
1
2
w
âŠºw +
1
2
C Î¾
âŠºÎ¾
L2 loss
L

w, b, Ï, Î¾

 C

=
1
2
w
âŠºw +
1
2
C
Xn
j=1
Î¾
2
j +
1
2
b
2 âˆ’ Ï =
1
2
w
âŠºw +
1
2
C Î¾
âŠºÎ¾ +
1
2
b
2 âˆ’ Ï
note: terminology varies from author to author . . .
16 / 85
question
where do these come from ???
answer
it is actually not quite clear, but . . .
LSQ SVMs can be traced back to work by Suykens and Vanderwalle (1999)
L2 SVMs can be traced back to work by FrieÃŸ and Harrison (1998) and by
Mangasarian and Musicant (2001)
17 / 85
least squares SVMs
18 / 85
primal problem of least squares SVM training
argmin
w, b, Î¾
1
2

w
âŠºw + C Î¾
âŠºÎ¾

s.t. y = X
âŠºw + b 1 + Î¾
19 / 85
Lagrangian
L

w, b, Î¾, Î»

=
1
2
w
âŠºw +
1
2
C Î¾
âŠºÎ¾ + Î»
âŠº

y âˆ’ X
âŠºw âˆ’ b 1 âˆ’ Î¾

=
1
2
w
âŠºw +
1
2
C Î¾
âŠºÎ¾ + Î»
âŠº
y âˆ’ Î»
âŠºX
âŠºw âˆ’ b Î»
âŠº
1 âˆ’ Î»
âŠºÎ¾
20 / 85
KKT conditions
KKT 1
0
!
=
âˆ‚L
âˆ‚w
= w âˆ’ X Î» â‡’ w = X Î»
0
!
=
âˆ‚L
âˆ‚Î¾
= C Î¾ âˆ’ Î» â‡’ Î¾ =
1
C
Î»
.
.
.
â‡’ we can eliminate w and Î¾ from L to get
L

b, Î»

=
1
2
Î»
âŠºX
âŠºX Î» +
1
2C
Î»
âŠºÎ» + Î»
âŠº
y âˆ’ Î»
âŠºX
âŠºX Î» âˆ’ b Î»
âŠº
1 âˆ’
1
C
Î»
âŠºÎ»
= âˆ’
1
2
Î»
âŠº

X
âŠºX +
1
C
I

Î» + Î»
âŠº
y âˆ’ b Î»
âŠº
1
21 / 85
â‡’ KKT 1
0
!
=
âˆ‚L
âˆ‚b
= âˆ’1
âŠºÎ» â‡’ 1
âŠºÎ» = 0
0
!
=
âˆ‚L
âˆ‚Î»
= âˆ’
X
âŠºX +
1
C
I

Î» + y âˆ’ b 1 â‡’ b 1 +

X
âŠºX +
1
C
I

Î» = y
â‡’ we have 2 equations in 2 unknowns, namely

X
âŠºX +
1
C
I

Î» + 1 Â· b = y
1
âŠºÎ» + 0 Â· b = 0
22 / 85
â‡’ we have

X
âŠºX +
1
C
I 1
1
âŠº
0
 Î»
b

=

y
0

â‡”

Î»
b

=

X
âŠºX +
1
C
I 1
1
âŠº
0
âˆ’1 
y
0

from this and KKT 1, we immediately get w = X Î»
â‡” we donâ€™t need to bother with the dual problem
but letâ€™s do it anyway . . .
23 / 85
we simply plug
1
âŠºÎ» = 0
into L

b, Î»

to get
L

b, Î»

= âˆ’
1
2
Î»
âŠº

X
âŠºX +
1
C
I

Î» + Î»
âŠº
y âˆ’ b Î»
âŠº
1
= âˆ’
1
2
Î»
âŠº

X
âŠºX +
1
C
I

Î» + Î»
âŠº
y
= D

Î»

we thus obtain the . . .
24 / 85
dual problem of least squares SVM training
argmin
Î»
âˆ’
1
2
Î»
âŠº
h
X
âŠºX +
1
C
I
i
Î» âˆ’ y
âŠºÎ»
s.t. 1
âŠºÎ» = 0
25 / 85
note
again, we do not really need to solve the dual because we can easily solve the primal
however, computing the primal solution requires us to invert a potentially large matrix
in very big data settings, the dual problem formulation may therefore have its merits
alas, while solving it is not really rocket science, its sum-to-zero constraint is unwieldy
so, letâ€™s look at â€œthe real contenderâ€ . . .
26 / 85
L2 SVMs
27 / 85
primal problem of L2 SVM training
argmin
w, b, Ï, Î¾
1
2

w
âŠºw + b
2 + C Î¾
âŠºÎ¾

âˆ’ Ï
s.t. yj

w
âŠº
xj âˆ’ b

â©¾ Ï âˆ’ Î¾j
, j = 1, . . . , n
28 / 85
primal problem of L2 SVM training (compact form)
argmin
w, b, Ï, Î¾
1
2

w
âŠºw + b
2 + C Î¾
âŠºÎ¾

âˆ’ Ï
s.t. Z
âŠºw âˆ’ b y â©¾ Ï 1 âˆ’ Î¾
29 / 85
Lagrangian
L

w, b, Ï, Î¾, Âµ

=
1
2

w
âŠºw + C Î¾
âŠºÎ¾ + b
2

âˆ’ Ï âˆ’
Xn
j=1
Âµj

yj

w
âŠº
xj âˆ’ b

âˆ’ Ï + Î¾j

=
1
2

w
âŠºw + C Î¾
âŠºÎ¾ + b
2

âˆ’ Ï âˆ’ w
âŠºZ Âµ + b y
âŠºÂµ + Ï 1
âŠºÂµ âˆ’ Î¾
âŠºÂµ
30 / 85
KKT conditions
KKT 1
0
!
=
âˆ‚L
âˆ‚w
= w âˆ’ Z Âµ â‡’ w = Z Âµ
0
!
=
âˆ‚L
âˆ‚b
= b + y
âŠºÂµ â‡’ b = âˆ’y
âŠºÂµ
0
!
=
âˆ‚L
âˆ‚Î¾
= C Î¾ âˆ’ Âµ â‡’ Î¾ =
1
C
Âµ
0
!
=
âˆ‚L
âˆ‚Ï = âˆ’1 + 1
âŠºÂµ â‡’ 1
âŠºÂµ = 1
31 / 85
â‡’ we can eliminate w, b, Ï, and Î¾ from the Lagrangian
L

w, b, Ï, Î¾, Âµ

=
1
2

w
âŠºw + C Î¾
âŠºÎ¾ + b
2

âˆ’ Ï âˆ’ w
âŠºZ Âµ + b y
âŠºÂµ + Ï 1
âŠºÂµ âˆ’ Î¾
âŠºÂµ
=
1
2
Âµ
âŠºZ
âŠºZ Âµ +
1
2C
Âµ
âŠºÂµ +
1
2
b
2 âˆ’ Ï âˆ’ Âµ
âŠºZ
âŠºZ Âµ âˆ’ b
2 + Ï âˆ’
1
C
Âµ
âŠºÂµ
= âˆ’
1
2
Âµ
âŠºZ
âŠºZ Âµ âˆ’
1
2C
Âµ
âŠºÂµ âˆ’
1
2
b
2
= âˆ’
1
2
Âµ
âŠºZ
âŠºZ Âµ âˆ’
1
2C
Âµ
âŠºÂµ âˆ’
1
2
Âµ
âŠº
yyâŠºÂµ
= âˆ’
1
2
Âµ
âŠº

Z
âŠºZ + yyâŠº +
1
C
I

Âµ
= D

Âµ

but have to keep in mind the following two constraints
1
âŠºÂµ = 1 (from KKT 1 above)
Âµ â©¾ 0 (from KKT 3)
32 / 85
dual problem of L2 SVM training
argmin
Âµ
Âµ
âŠº
h
Z
âŠºZ + yyâŠº +
1
C
I
i
Âµ
s.t.
1
âŠºÂµ = 1
Âµ â©¾ 0
33 / 85
dual problem of L2 SVM training (even more compact)
argmin
Âµâˆˆâˆ†nâˆ’1
Âµ
âŠº
h
Z
âŠºZ + yyâŠº +
1
C
I
i
Âµ
34 / 85
note
using the Frank-Wolfe algorithm, this dual is easily solved
â‡” for those â€œin the knowâ€, training an L2 SVM is but a breeze
all we need are 13 lines of plain vanilla numpy code
(or even fewer if we were to sacrifice readability) . . .
35 / 85
numpy
def trainL2SVM(matX, vecY, C=1., T=10_000):
m, n = X.shape
matI = np.eye(n)
matZ = matX * vecY
matM = matZ.T @ matZ + np.outer(vecY, vecY) + matI / C
vecM = np.ones(n) / n
for t in range(T):
beta = 2 / (t+2)
grad = 2 * matM @ vecM
vecM += beta * (matI[np.argmin(grad)] - vecM)
vecW = matZ @ vecM
b = -vecY @ vecM
return vecW, b
note: in practice, parameters C and T may need tuning
36 / 85
examples (same model sign(w
âŠºx âˆ’ b), different training objectives / algorithms)
perceptron naÂ¨Ä±ve LSQ LDA L2 SVM
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
37 / 85
limitations of linear models
38 / 85
if the given training data are either perfectly- or â€œalmostâ€ linearly separable,
then . . .
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
39 / 85
if the given training data are either perfectly- or â€œalmostâ€ linearly separable,
then linear SVMs are very robust binary classifiers . . .
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
âˆ’4 âˆ’2 0 2 4 6 8 10
âˆ’2
0
2
4
6
8
10
39 / 85
but what if we were dealing with clearly non-linearly separable data ???
âˆ’2 0 2
âˆ’2
0
2
âˆ’1 1
âˆ’1
1
âˆ’5 âˆ’3 âˆ’1 1 3
âˆ’4
âˆ’2
0
2
4
40 / 85
1st possible strategy
we could consider / learn an ensemble of weak (linear) classifiers . . .
âˆ’2 âˆ’1 0 1 2
âˆ’2
âˆ’1
0
1
2
f1(x) = sign
w
âŠº
1
x âˆ’ b1

âˆ’2 âˆ’1 0 1 2
âˆ’2
âˆ’1
0
1
2
f2(x) = sign
w
âŠº
2
x âˆ’ b2

âˆ’2 âˆ’1 0 1 2
âˆ’2
âˆ’1
0
1
2
f3(x) = sign
w
âŠº
3
x âˆ’ b3

âˆ’2 âˆ’1 0 1 2
âˆ’2
âˆ’1
0
1
2
f4(x) = sign
w
âŠº
4
x âˆ’ b4

41 / 85
in order to train a strong classifier
F

x

= sign X
i
Wi
fi(x) âˆ’ B
!
âˆ’2 âˆ’1 0 1 2
âˆ’2
âˆ’1
0
1
2
F

x

= sign
WâŠº
f(x) âˆ’ B

42 / 85
note
this is what boosting algorithms do
AdaBoost, BrownBoost, LogitBoost, . . .
this is also what (shallow) neural networks do
x1 x2
. . . xm
1 f1 f2 f3
. . . fM
F
43 / 85
2nd possible strategy
we could devise a non-linear feature map Ï† : R
m â†’ R
M
Ï†(x) =
ï£®
ï£°
x1
x2
x
2
1 + x
2
2
ï£¹
ï£»
âˆ’2 âˆ’1 0 1 2
âˆ’2
âˆ’1
0
1
2
x âˆˆ R
2
Ï†â†’
Ï†x âˆˆ R
3
44 / 85
principle of machine learning
just as the above (didactic) example suggests, non-linear transformations
Ï† : R
m â†’ R
M
into high dimensional spaces (M â‰« m) can make data linearly separable
â‡’ if we could find an appropriate transformation, we could train a classifier
g(x) = sign
Ï‰âŠºÏ†x âˆ’ Î²

with parameters Ï‰ âˆˆ R
M, Î² âˆˆ R
45 / 85
note
there are at least three fundamental problems with this strategy
1) it is not generally clear what transformation Ï†(Â·) to choose
2) depending on the dimension of the target space R
M, it may
be expensive to compute Ï†x = Ï†(x) for x âˆˆ R
m
3) depending on the dimension of the target space R
M, it may
be expensive to compute inner products Ï‰âŠºÏ†x
but Mercerâ€™s theorem comes to the rescue . . .
46 / 85
the kernel trick
47 / 85
Mercer kernel
a Mercer kernel is a positive semidefinite function
k : R
m Ã— R
m â†’ R
for which there exists Ï† : R
m â†’ R
M such that
k

x, y

= Ï†
âŠº
x Ï†y = Ï†
âŠº
y Ï†x = k

y, x

48 / 85
we say that a function k : R
m Ã— R
m â†’ R is positive semidefinite, if for all continuous f : R
m â†’ R
x
f (x) k(x, y) f ( y) dx dy â©¾ 0
this generalizes the concept of positive semidefinite matrices M âˆˆ R
mÃ—m where for all v âˆˆ R
m
v
âŠºM v =
X
i
X
j
vi Mij vj â©¾ 0
49 / 85
positive semidefinite matrices have orthogonal eigenvectors ui and positive eigenvalues Î»i > 0,
i = 1, . . . , m such that
M ui = Î»i ui
u
âŠº
i
uj = Î´ij
positive semidefinite functions have orthogonal eigenfunctions Ïˆi(x) and eingenvalues Âµi > 0,
i = 1, . . . , âˆ such that
Z
k(x, y) Ïˆi( y) dy = Âµi Ïˆi(x)
Z
Ïˆi(x) Ïˆj(x) dx = Î´ij
50 / 85
â‡’ in analogy to the spectral representation of a p.s.d. matrix
M =
Xm
i=1
Î»i ui u
âŠº
i
we also have a spectral representation of a p.s.d. function
k(x, y) = Xâˆ
i=1
Âµi Ïˆi(x) Ïˆi( y)
51 / 85
theorem (Mercer, 1909)
for every positive semidefinite function k(Â·, Â·),
there exists a vector valued function Ï†(Â·) s.t.
k

x, y

= Ï†(x)
âŠºÏ†( y)
52 / 85
proof (ignoring dirty details)
k

x, y

=
X
i
Âµi Ïˆi(x) Ïˆi( y)
=
X
i
âˆš
Âµi Ïˆi(x)
âˆš
Âµi Ïˆi( y)
â‰¡
X
i
Ï†i(x) Ï†i( y)
= Ï†(x)
âŠºÏ†( y)
53 / 85
note
w.r.t. the three problems we listed above, we have the following implications
3) instead of computing Ï†
âŠº
x Ï†y on vectors in R
M, we may evaluate a kernel
function k(x, y) on vectors in R
m
2) we may not have to compute Ï†x and Ï†y explicitly, but may directly evaluate
a kernel function k(x, y)
1) instead of worrying about how to choose Ï†(Â·), we may worry about choosing
a suitable kernel function k(Â·, Â·)
54 / 85
the kernel trick
the kernel trick is
first, to rewrite an algorithm for data analysis in
such a way that input data only appears in form
of inner products with other data
second, to replace any occurrence of such inner
products by kernel evaluations
this way, we can use linear models for non-linear tasks
55 / 85
we will see this trick in action soon . . .
but first, we will familiarize ourselves with kernel functions
and look at the most popular kernels in machine learning
56 / 85
kernel engineering
57 / 85
linear kernel
â‡” proof that there exists at least one instance of a valid Mercer kernel
the identity mapping Ï†(x) = x gives rise to a valid kernel function
k

x, y

= Ï†(x)
âŠºÏ†( y) = x
âŠº
y
because we have symmetry
x
âŠº
y = y
âŠº
x
and positive semidefiniteness
x
âŠº
x â©¾ 0
58 / 85
claim
if we assume
x, y âˆˆ R
m
b > 0 âˆˆ R
c > 0 âˆˆ R
d > 0 âˆˆ N
g : R
m â†’ R
ki(x, y) is a valid kernel
then all these are valid kernels
k(x, y) = c Â· k1(x, y)
k(x, y) = g(x) k1(x, y) g( y)
k(x, y) = x
âŠº
y + b
k(x, y) = k1(x, y) + k2(x, y)
k(x, y) =
k1(x, y)
d
k(x, y) =
x
âŠº
y + b
d
k(x, y) = exp
âˆ’
1
2Ïƒ2

x âˆ’ y


2

here are some proofs . . .
59 / 85
inhomogeneous linear kernel
the inhomogeneous linear kernel
k

x, y

= b + x
âŠº
y
is a valid kernel, because we may define
Ï†(x) = âˆš
b
x

âˆˆ R
m+1
so that
Ï†(x)
âŠºÏ†( y) = âˆš
b Â·
âˆš
b + x
âŠº
y = b + x
âŠº
y = k

x, y

60 / 85
note that we may alternatively define
Ï‘(x) = âˆš
b
Ïˆ(x) = x
Ï†(x) = Ï‘(x) âŠ• Ïˆ(x) = 
Ï‘(x)
Ïˆ(x)

âˆˆ R
m+1
from which we obtain the following chain of equalities
Ï†(x)
âŠºÏ†( y) = Ï‘(x)
âŠºÏ‘( y) + Ïˆ(x)
âŠºÏˆ( y) = k1

x, y

+ k2

x, y

= k

x, y

and thus see an example for the fact that the sum of two valid kernels is another valid kernel
61 / 85
polynomial kernel
the polynomial kernel
k

x, y

=

b + x
âŠº
y
d
is a valid kernel, because â€œone can showâ€ that the multinomial expansion
k

x, y

=
X
Pm+1
j
i=1
ji=d

d
j1, . . . , jm+1
Ym
t=1
x
ji
t y
ji
t
Â· b
jm+1
2 b
jm+1
2
is an inner product between two vectors in an m+d
d

dimensional space
due to the horrible combinatorics, we will not do this but only illustrate the . . .
62 / 85
very special case: m = 2, d = 2
k

x, y

=

b + x
âŠº
y
2 =

b + x1y1 + x2y2
2 = x
2
1
y
2
1 + x
2
2
y
2
2 + 2 x1x2y1y2 + b
2 + 2 bx1y1 + 2 bx2y2
â‡’ we can express this as
k

x, y

= Ï†(x)
âŠºÏ†(y)
where
Ï†(x) =
ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°
x
2
1
x
2
âˆš 2
2
âˆšx1x2
âˆš
b
âˆš
2 b x1
2 b x2
ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»
âˆˆ R(
4
2) = R
6
63 / 85
Gaussian kernel
the Gaussian kernel
k

x, y

= e
âˆ’
âˆ¥xâˆ’yâˆ¥
2
2Ïƒ2
is a valid kernel, because
k

x, y

= e
âˆ’ xâŠºx
2Ïƒ2 +
xâŠºy
Ïƒ2 âˆ’
yâŠºy
2Ïƒ2 = e
âˆ’ xâŠºx
2Ïƒ2
Â· e
xâŠºy
Ïƒ2
Â· e
âˆ’
yâŠºy
2Ïƒ2 = g

x

Â· k1

x, y

Â· g

y

for which we observe that
k1

x, y

= e
1
Ïƒ2
x
âŠºy =
Xâˆ
r=0

1
Ïƒ2 x
âŠº
y
r
r!
â‡’ the Gaussian kernel is essentially in infinite sum over polynomial kernels
64 / 85
take home message
kernel functions k(x, y) implicitly compute inner products Ï†(x)
âŠºÏ†( y)
not in the data space R
m but in a latent feature space R
M
k

x, y

M
inhom. linear kernel x
âŠº
y + b m + 1
polynomial kernel
x
âŠº
y + b
d
m+d
d

Gaussian kernel e
âˆ’
âˆ¥xâˆ’yâˆ¥
2
2Ïƒ2 âˆ
65 / 85
kernel SVMs
66 / 85
historically, SVMs were the first machine learning technology that was kernelized
this is because their very nature allows for working with kernels
during training, all training data points exclusively occur within inner products
during application, all input data points exclusively occur within inner products
â‡’ we can replace these inner products by kernel evaluations w/o rewriting the algorithm
letâ€™s again focus on LSQ- and L2 SVMs . . .
67 / 85
kernel SVM training
SVM training involves matrices which we may kernelize as follows
LSQ SVM 
X
âŠºX +
1
C
I 1
1
âŠº
0
âˆ’1
âˆ’â†’ 
K +
1
C
I 1
1
âŠº
0
âˆ’1
L2 SVM h
X
âŠºX âŠ™ yyâŠº + yyâŠº +
1
C
I
i
âˆ’â†’ h
K âŠ™ yyâŠº + yyâŠº +
1
C
I
i
68 / 85
â‡’ we replace the Gram matrix X
âŠºX whose entries are inner products

X
âŠºX

ij = x
âŠº
i
xj
by a kernel matrix K whose entries are kernel evaluations

K

ij = k

xi
, xj

69 / 85
kernel SVM application
SVM application involves vectors which we may kernelize as follows
LSQ SVM x
âŠºX Î» âˆ’â†’ k(x)
âŠºÎ»
L2 SVM h
y
âŠº âŠ™ x
âŠºX
i
Âµ âˆ’â†’ h
y
âŠº âŠ™ k(x)
âŠº
i
Âµ
70 / 85
â‡’ we replace the vector x
âŠºX whose entries are inner products

x
âŠºX

j
= x
âŠº
xj
by a kernel vector k(x)
âŠº whose entries are kernel evaluations

k(x)

j
= k

x, xj

71 / 85
one more thing . . .
if S âŠ† X =

x1, x2, . . . xn
	
is the set of support vectors and xj Ì¸âˆˆ S, then
Î»j = 0 (for an LSQ SMV)
Âµj = 0 (for an L2 SMV)
â‡’ during application, we can often save a lot of compute time because
k(x)
âŠºÎ» =
X
xjâˆˆX
Î»j k

x, xj

=
X
xsâˆˆS
Î»s k

x, xs

(for an LSQ SMV)
h
y
âŠº âŠ™ k(x)
âŠº
i
Âµ =
X
xjâˆˆX
Âµj yj k

x, xj

=
X
xsâˆˆS
Âµs ys k

x, xs

(for an L2 SMV)
72 / 85
example
non-linear decision functions learned by kernel L2 SMVs with Gaussian kernels
73 / 85
observe
this flexibility comes at a cost â‡” more support vectors than in the linear case
74 / 85
question
how were these figures created ?
answer
as a Christmas present, weâ€™ll show you . . .
75 / 85
first of all, we implemented two functions
def gaussKernelMatrix(matXtrn, **kPars):
...
def gaussKernelVector(matXtst, matXtrn, **kPars):
...
gaussKernelMatrix takes Xtrn âˆˆ R
mÃ—n and returns K âˆˆ R
nÃ—n with

K

ij = e
âˆ’ 1
2Ïƒ2 âˆ¥xiâˆ’xjâˆ¥
2
gaussKernelVector takes Xtst âˆˆ R
mÃ—ntst and Xtrn âˆˆ R
mÃ—ntrn where
ntst can actually be only 1 and returns a corresponding K âˆˆ R
ntstÃ—ntrn
76 / 85
here is how we implemented gaussKernelVector
import scipy.spatial as spt
def SQEDMAB(A,B):
"""
if matrices A and B are of shapes (m, na) and (m, nb), then this
functions returns a squared distance matrix of shape (na, nb)
"""
return spt.distance.cdist(A.T, B.T, â€™sqeuclideanâ€™)
def gaussKernelVector(matXtst, matXtrn, **kPars):
sigm = kPars[â€™sigmâ€™] if â€™sigmâ€™ in kPars else 1
if matXtst.ndim == 1:
dist = np.sum((matXtrn.T-matXtst)**2, axis=1)
else:
dist = SQEDMAB(matXtst, matXtrn)
return np.exp(-0.5/sigm**2 * dist)
77 / 85
training a kernel L2 SVM is easy
### training data points and labels
matX = ...
vecY = ...
### kernel parameters and functions
kParams = {â€™sigmâ€™ : 2.}
kMatFct = gaussKernelMatrix
kVecFct = gaussKernelVector
### compute support vectors, their labels, and Lagrange multipliers
matXs, vecYs, vecMs = trainKernelL2SVM(matX, vecY, kMatFct, kParams, C=100.)
where . . .
78 / 85
def trainKernelL2SVM(matX, vecY, kFct, kPars, C=1., T=10_000):
matK = kFct(matX, **kPars)
_, n = matK.shape
matI = np.eye(n)
matY = np.outer(vecY, vecY)
matM = matK * matY + matY + matI / C
vecM = np.ones(n) / n
for t in range(T):
beta = 2 / (t+2)
grad = matM @ vecM
vecM += beta * (matI[np.argmin(grad)] - vecM)
### return support vectors, their labels and Lagarange multipliers
return matX[:,vecM>0], vecY[vecM>0], vecM[vecM>0]
79 / 85
here is how we computed the learned decision function
f(x) = sign
y
âŠº âŠ™ k(x)
âŠº

Âµ + y
âŠºÂµ

= sign X
xsâˆˆS
Âµs ys k

x, xs

+ Âµs ys
!
### extract class specific training data for plotting
X1 = matX[:,vecY<0]
X2 = matX[:,vecY>0]
### compute bounding box of training data for plotting
bbox = compBBox(matX)
### compute decision function for plotting
decFnct = compDecFnct(matXs, vecYs, vecMs, kVecFct, kParams, bbox)
compBBox will be provided in the exercise and . . .
80 / 85
def compDecFnct(matX, vecY, vecM, kFct, kPars, bbox, nx=512):
w = bbox[â€™xmaxâ€™] - bbox[â€™xminâ€™]
h = bbox[â€™ymaxâ€™] - bbox[â€™yminâ€™]
ny = int(nx*h/w)
xs, ys = np.meshgrid(np.linspace(bbox[â€™xminâ€™], bbox[â€™xmaxâ€™], nx),
np.linspace(bbox[â€™yminâ€™], bbox[â€™ymaxâ€™], ny))
matXtst = np.vstack((xs.flatten(), ys.flatten()))
vecKtst = kFct(matXtst, matX, **kPars)
vecYtst = np.sign(np.sum(vecKtst * vecY * vecM, axis=1) + vecY @ vecM)
### return a triple of arrays of pixel x- and y-coordinates
### and of the correspponding function values f(x,y)
return xs, ys, vecYtst.reshape(ny, nx)
81 / 85
for visualization, we used plot2dDataFnct which we will provide in the exercises
calling it like this
plot2dDataFnct([X1, X2], bbox, showAxes=True, filename=â€™xmpl1.pdfâ€™)
plot2dDataFnct([X1, X2], bbox, fctF=decFnct, showAxes=True, filename=â€™xmpl2.pdfâ€™)
plot2dDataFnct([X1, X2], bbox, fctF=decFnct, showAxes=False, showCont=True, filename=â€™xmpl3.pdfâ€™)
creates figures like these
âˆ’2 0 2 4 6
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
4
âˆ’2 0 2 4 6
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
4
82 / 85
note
all of this involved just a single for loop (for the Frank-Wolfe iterations)
â‡” if you know what you are doing (â‡” if you are a numpy black-belt), then
your machine learning code will run in mere fractions of a second
otherwise, your code will run for minutes or even hours which is not good
take these points seriously because AI code generators certainly do . . .
83 / 85
summary
84 / 85
we now know about
further support vector machines for binary classification of
linearly separable data
non linearly separable data
further primal and dual forms of the SVM training problem
an utterly simple implementation of SVM training based on
the Frank-Wolfe algorithm
85 / 85