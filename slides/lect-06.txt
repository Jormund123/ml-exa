Principles of Machine Learning
Prof. Christian Bauckhage
1 / 87
lecture 06
probabilistic model fitting (part 2)
in this lecture, we continue our study of probabilistic model fitting
we first apply Bayesian methods to fit a multivariate Gaussian to multivariate data
then, we revisit the regression problem and approach it from the points of view of
maximum likelihood- and Bayesian parameter estimations
next, we also revisit logistic regression classifiers and show that these, too, come
with a probabilistic interpretation
finally, we show that even tanh-neurons can be seen as Bayesian decision makers
2 / 87
outline
recap
probabilistic model fitting (2)
(dis)advantages of Bayesian parameter estimation
regression revisited
least squares
maximum likelihood
Bayesian regression
logistic regression revisited
summary
3 / 87
recap
4 / 87
probabilistic modeling
given data D =

xj
	n
j=1
, we hypothesize
it has been sampled from a distribution
p

x

 Î¸

if we assume the data in D are i.i.d., then
p

D

 Î¸

= p

x1, . . . , xn

 Î¸

=
Yn
j=1
p

xj

 Î¸

5 / 87
probabilistic model fitting
given D and our model hypothesis p

x

 Î¸

, we
want to estimate â€œoptimalâ€ model parameters Î¸Ë†
the maximum likelihood estimator of Î¸ is
Î¸ML = argmax
Î¸
p

D

 Î¸

the maximum a-posteriori estimator of Î¸ is
Î¸MAP = argmax
Î¸
p

Î¸

 D

6 / 87
the likelihood function
L

Î¸

â‰¡ L

Î¸

 D

= p

D

 Î¸

= p

x1, . . . , xn

 Î¸

=
Yn
j=1
p

xj

 Î¸

is understood as a function of Î¸ â‡” is not a probability density
for convenience and numerical stability, we typically work with . . .
the log-likelihood function
L

Î¸

= log L

Î¸

= log p

x1, . . . , xn

 Î¸

= logYn
j=1
p

xj

 Î¸

=
Xn
j=1
log p

xj

 Î¸

7 / 87
the MLEs of the parameters of a Gaussian N

x

 Âµ, Î£

are
ÂµML =
1
n
Xn
j=1
xj
Î£ML =
1
n
Xn
j=1

xj âˆ’ ÂµMLxj âˆ’ ÂµMLâŠº
while ÂµML is an unbiased estimator, Î£ML is a biased estimator
E

ÂµML
= Âµ â‡” bias
ÂµML
= E

Âµ âˆ’ ÂµML
= 0
E

Î£ML
=
n âˆ’ 1
n
Î£ â‡” bias
Î£ML
= E

Î£ âˆ’ Î£ML
Ì¸= 0
8 / 87
we further have this crucial terminology
p

Î¸

 D

=
p

D

 Î¸

Ã— p

Î¸

p

D
 â‡” posterior =
likelihood Ã— prior
evidence
if prior p

Î¸

and posterior p

Î¸

 D

are from the
same probability distribution family, then we say
p

Î¸

and p

Î¸

 D

are conjugate probability distributions
p

Î¸

is a conjugate prior of the the likelihood p

D

 Î¸

p

Î¸

 D

is a reproducing probability distribution
9 / 87
conjugate priors are mainly a mathematical convenience
â‡” they may not really reflect â€œrelevantâ€ prior knowledge, but . . .
they typically tremendously simplify the model fitting process
for instance . . .
10 / 87
the evidence
p

D

=
Z
p

D, Î¸

dÎ¸ =
Z
p

D

 Î¸

p

Î¸

dÎ¸
(often) has a closed form solution if p

Î¸

is a conjugate prior of p

D

 Î¸

the posterior predictive distribution
p

x

 D

=
Z
p

x

 Î¸

p

Î¸

 D

dÎ¸ âˆ
Z
p

x

 Î¸

p

D

 Î¸

p

Î¸

dÎ¸
(often) has a closed form solution if p

Î¸

is a conjugate prior of p

D

 Î¸

11 / 87
people love Gaussian models
p

x

 Î¸

= N

x

 Âµ, Î£

= N

x

 Âµ, Î›âˆ’1

mainly because working with Gaussians is a breeze (we will see further examples below)
however, note the following
+ for natural data (observations from physical / chemical / biological processes),
Gaussians are often appropriate models (there are deep reasons for this)
âˆ’ for human-made data (e.g. income distributions, stock prices, voting behavior),
Gaussians are often inappropriate models (there are deep reasons for this)
12 / 87
probabilistic model fitting (2)
13 / 87
fitting a multi-variate Gaussian
finally, we consider multivariate data D =

xj
	n
j=1
and fit a multivariate Gaussian p

x

= N

x

 Âµ, Î£

for which mean Âµ and covariance Î£ are unknown
in settings like this, it is again preferable to work with the precision matrix Î› = Î£
âˆ’1
when parameterized in terms of the precision matrix, our Gaussian model reads
p

x

= N

x

 Âµ, Î›âˆ’1

14 / 87
for simplicity, we assume a conjugate prior distribution which is the Normal-Wishart
p

Âµ, Î›

= NW
Âµ, Î›

 Âµ0, Îº, W, Î½

= N

Âµ

 Âµ0,
1
Îº Î›âˆ’1

Â· W

Î›

 W, Î½

which involves the Wishart distribution
W

Î›

 W, Î½

=
1
Î“m(
Î½
2
)
vuut
det
Î›
Î½âˆ’mâˆ’1
2
Î½ m det
WÎ½
 exp 
âˆ’
1
2
tr
Wâˆ’1Î›


where m is the dimension of the xj âˆˆ R
m
15 / 87
given (reasonable) initial guesses Âµ0, W0, Îº0 and Î½0, we have the posterior
p

Âµ, Î›

 D

âˆ N

D

 Âµ, Î›âˆ’1

NW
Âµ, Î›

 Âµ0, Îº0, W0, Î½0

which is another Normal-Wishart distribution (and thus a reproducing density)
John Wishart
(âˆ—1898, â€ 1956)
16 / 87
going through the required motions, we find
Îºn = Îº0 + n
Î½n = n0 + n
Âµn =
Îº0 Âµ0 + n ÂµË†n
Îºn
Wâˆ’1
n = Wâˆ’1
0 + (n âˆ’ 1) Î£Ë†
n +
n Îº0
Îºn

Âµ0 âˆ’ ÂµË†n
Âµ0 âˆ’ ÂµË†n
âŠº
where we note the occurrence of the MLEs ÂµË†n and Î£Ë†
n
17 / 87
corresponding MAP estimators for the sought after Âµ and Î› amount to
ÂµMAP = argmax
Âµ
p

Âµ, Î›

 D

= Âµn
Î›MAP = argmax
Î›
p

Âµ, Î›

 D

= (Î½n âˆ’ m)Wn
given these two MAP estimators, our multi-variate data model becomes
p

x

= N

x

 Âµn,
Wâˆ’1
n
Î½nâˆ’m

18 / 87
illustration
data
150 160 170 180 190 200
height [cm]
20
40
60
80
100
120
weight [kg]
MLE model
150 160 170 180 190 200
height [cm]
20
40
60
80
100
120
weight [kg]
ÂµML â‰ˆ

175.7
73.9

Î£ML â‰ˆ

75.9 64.5
64.5 186.9

MAP model
150 160 170 180 190 200
height [cm]
20
40
60
80
100
120
weight [kg]
ÂµMAP â‰ˆ

175.4
73.8

Î£MAP â‰ˆ

71.1 59.1
59.1 168.6

19 / 87
observe: the last example we went through also allows for
closed forms of the posterior predictive distribution p

x

 D

without going into details, we therefore simply mention that
multi-variate Gaussian model with Normal-Wishart prior on (Âµ, Î›)
p

x

 D

is a multi-variate (three parameter) Studentâ€™s t-distribution
20 / 87
(dis)advantages of
Bayesian parameter estimation
21 / 87
note
+ the prior p(Î¸) allows for incorporating (domain specific) knowledge
Â± if no such knowledge is available, the prior is typically chosen such
that it is mathematically convenient
if the prior p(Î¸) is in the same family as the posterior p(Î¸ | D),
it is called a conjugate prior
we saw that conjugate priors â€œsimplifyâ€ computations because
they lead to closed form solutions
all densities in the exponential family come with conjugate priors
22 / 87
note
+ conjugate priors allow for closed form solutions of the posterior predictive density
posterior predictive densities can be very valuable (we shall see examples later)
âˆ’ representing prior knowledge in terms of (conjugate) prior distributions is anything
but intuitive and by and large a fantasy of proponents of the Bayesian framework
for example: what are good initial guesses of the parameters of a Gamma prior?
where do they come from? how are they justified? what is their significance?
23 / 87
note
âˆ’ Bayesian statistics does not allow for falsifying hypotheses
â‡” there is no false hypothesis, only different degrees of belief
if these are due to mathematical convenience, they may be questionable
âˆ’ if priors are not conjugate, then it is usually very difficult to compute the evidence
p

D

=
R
p(D | Î¸) p(Î¸) dÎ¸ (no closed form solutions, curse of dimensionality, . . . )
these difficulties may be tackled using, say, Markov Chain Monte Carlo (MCMC)
24 / 87
regression revisited
25 / 87
uni-variate linear regression
given a training data sample D = 
xj, yjnj=1
assume a noisy linear model yj = Ï†âŠºj w + Ïµj Ï†j = 1xj
estimate optimal parameters w = w0 w1
150 160 170 180 190 200
height [cm]
20
40
60
80
100
120
weight [kg]
26 / 87
1st approach: least squares
representing our training data as Î¦ = ï£®ï£°
| | | Ï†1 Ï†2 Â· Â· Â· Ï†
n
| | |
ï£¹ï£»
y
=

y
1
y
2 Â· Â· Â·
y
n

âŠº
and considering the squared loss Lw = Î¦âŠºw âˆ’ y2
we posit that optimal parameters
will minimize this loss
150 160 170 180 190 200
height [cm]
20
40
60
80
100
120
weight [kg]
27 / 87
â‡’ good parameters result from solving
wË† = argmin
w

Î¦âŠºw âˆ’ y


2
â‡’ the unique solution amounts to
wË† =

Î¦Î¦âŠº
âˆ’1Î¦ y
â‡’ our fitted model is given by
y = f

x

 wË†

= Ï†
âŠº
xwË†
150 160 170 180 190 200
height [cm]
20
40
60
80
100
120
weight [kg]
28 / 87
just to be clear . . .
we decided to model our data via a linear function
y = f

x

 w

= Ï†
âŠº
xw
to determine the best linear function w.r.t. our data
y = f

x

 wË†

= Ï†
âŠº
xwË†
we decided to minimize a squared error (between
model predictions and targets)
the resulting optimized parameters depend on the
given data, because
wË† =

Î¦Î¦âŠº
âˆ’1Î¦ y
29 / 87
2nd approach: maximum likelihood
if we assume identical Gaussian noise
âˆ€ j : Ïµj âˆ¼ N
0, Ïƒ
2

our linear model can also be written as
yj = Ï†
âŠº
j w + N

0, Ïƒ
2

150 160 170 180 190 200
height [cm]
20
40
60
80
100
120
weight [kg]
30 / 87
if we furthermore define
âˆ€ j : Âµj = Ï†
âŠº
j w
we may also think probabilistically
yj âˆ¼ p

yj

 Âµj
, Ïƒ
2

= N

yj

 Âµj
, Ïƒ
2

â‡” each of the given yj has been drawn
from a Gaussian distribution whose
mean Âµj depends on a given xj and
whose variance is Ïƒ
2
150 160 170 180 190 200
height [cm]
20
40
60
80
100
120
weight [kg]
31 / 87
just to be clear . . .
we decided to model our data D via a conditional probability
y âˆ¼ p

y

 Âµ(x), Ïƒ
2

where the value of y is conditioned on a function of x, namely
Âµ(x) = f

x

 w

= Ï†
âŠº
x w
where f happens to be the linear function considered above
â‡” we model our data in D in terms of a conditional probability
p

y


f(x | w), Ïƒ
2

= p

y

 x, w, Ïƒ
2

32 / 87
more specifically, our model is y âˆ¼ N y  f(x | w), Ïƒ2
â‡” for each
x, there is a uni-variate
Gaussian with mean Âµ(x) = f  x  w
these infinitely many Gaussians
all have the same variance
Ïƒ
2
fitting the model to the data in
D
can be done via MLE to get point
estimates
wML and
Ïƒ
2
ML of optimal
parameters
150
160 170 180 190 200
0
20 40 60 80 100 120
33 / 87
if the data in D are i.i.d., the likelihood for w is
L

w

 D

=
Yn
j=1
N

yj


f(xj
| w), Ïƒ
2

=
Yn
j=1
1
âˆš
2 Ï€ Ïƒ2
exp 
âˆ’
1
2 Ïƒ2

f(xj
| w) âˆ’ yj
2

its87
if the data in D are i.i.d., the likelihood for w is
L

w

 D

=
Yn
j=1
N

yj


f(xj
| w), Ïƒ
2

=
Yn
j=1
1
âˆš
2 Ï€ Ïƒ2
exp 
âˆ’
1
2 Ïƒ2

f(xj
| w) âˆ’ yj
2

its log-likelihood is an arguably simpler function
L

w

 D

= âˆ’n log âˆš
2 Ï€ Ïƒ2 âˆ’
1
2 Ïƒ2
Xn
j=1

f(xj
| w) âˆ’ yj
2
34 / 87
note
maximizing L(w) â‡” minimizing âˆ’L(w)
wML = argmin
w
n log âˆš
2 Ï€ Ïƒ2 +
1
2 Ïƒ2
Xn
j=1

f(xj
| w) âˆ’ yj
2
= argmin
w
1
2 Ïƒ2
Xn
j=1

f(xj
| w) âˆ’ yj
2
= argmin
w
Xn
j=1

f(xj
| w) âˆ’ yj
2
= argmin
w
Xn
j=1

Ï†
âŠº
j w âˆ’ yj
2
= ar 87
note
maximizing L(w) â‡” minimizing âˆ’L(w)
wML = argmin
w
n log âˆš
2 Ï€ Ïƒ2 +
1
2 Ïƒ2
Xn
j=1

f(xj
| w) âˆ’ yj
2
= argmin
w
1
2 Ïƒ2
Xn
j=1

f(xj
| w) âˆ’ yj
2
= argmin
w
Xn
j=1

f(xj
| w) âˆ’ yj
2
= argmin
w
Xn
j=1

Ï†
âŠº
j w âˆ’ yj
2
= argmin
w

Î¦âŠºw âˆ’ y


2
35 / 87
note
â‡’ we realize that
least squares
â‡•
maximum likelihood
with a Gaussian noise model
36 / 87
since finding the maximum likelihood estimator wML of parameter w of our model N

y


f (x | w), Ïƒ2

is the same problem as finding the minimizer wË† of the squared error

Î¦âŠºw âˆ’ y


2
, we therefore have
wML =
Î¦Î¦âŠº
âˆ’1Î¦y
given wML, we can next compute the maximum likelihood estimator Ïƒ2
ML of parameter Ïƒ2 of our model
Ïƒ
2
ML = argmin
Ïƒ2
n log âˆš
2 Ï€ Ïƒ2 +
1
2 Ïƒ2

Î¦âŠºwML âˆ’ y


2
37 / 87
we may proceed as follows
âˆ‚
âˆ‚Ïƒ2

n log âˆš
2 Ï€ Ïƒ2 +
1
2 Ïƒ2

Î¦âŠºwML âˆ’ y


2

!= 0
â‡”
n
2 Ïƒ2 âˆ’
1
2 Ïƒ4

Î¦âŠºwML âˆ’ y


2 != 0
â‡”
n
2 Ïƒ2 =
1
2 Ïƒ4

Î¦âŠºwML âˆ’ y


2
from which we conclude that
Ïƒ
2
ML =
1
n

Î¦âŠºwML âˆ’ y


2
38 / 87
note
â‡’ we realize that
the squared error

Î¦âŠºw âˆ’ y


2
= n Ïƒ
2
is a (scaled) variance
â‡•
least squares minimizes a variance
39 / 87
once we have fitted our model
y âˆ¼ N
y


f(x | wML), Ïƒ
2
ML
we can next ask the following
question
given any x, which y can we expect our model to predict ?
40 / 87
answer
E

y

 x

=
+
Zâˆ
âˆ’âˆ
y Â· âˆš
1
2 Ï€ Ïƒ2
ML
exp "
âˆ’

y âˆ’ f(x | wML)
2
2 Ïƒ
2
ML #
dy
.
.
.
= f

x

 wML
41 / 87
note
â‡’ while it may now be self-evident, we emphasize that
a linear model
y = Ï†
âŠº
xw
computes the expectation
of a Gaussian model
y âˆ¼ N
y

 Ï†
âŠº
xw, Ïƒ
2

42 / 87
3rd approach: Bayesian estimation
given training data
D =


xj
, yj
n
j=1
we next use Bayesian estimation to fit a Gaussian model
yj âˆ¼ N
yj

 Ï†
âŠº
j w, Ïƒ
2

this will allow us to get a maximum a-posteriori estimate
wMAP of w and a posterior predictive distribution p

y

 x, D

43 / 87
for simplicity, we assume Ïƒ
2
is known, only w is unknown
â‡” we only require a prior distribution p

w

for parameter w
for simplicity, we choose a conjugate prior â‡” a Gaussian
p

w

= N

w

 Âµ0, Î£0

44 / 87
if we would go through all the motions (which we wonâ€™t),
we would find the posterior to be another Gaussian
p

w

 D

= N

w

 Âµn, Î£n

where
Î£n =
h
Î£
âˆ’1
0 +
1
Ïƒ2 Î¦Î¦âŠº
iâˆ’1
Âµn = Î£n
h
Î£
âˆ’1
0 Âµ0 +
1
Ïƒ2 Î¦y
i
45 / 87
since the mode of a Gaussian coincides with its mean,
we would have this maximum a-posteriori estimator
wMAP = argmax
w
N

w

 Âµn, Î£n

= Î£n
h
Î£
âˆ’1
0 Âµ0 +
1
Ïƒ2 Î¦y
i
but letâ€™s simplify things . . .
46 / 87
next, we focus on the (important) special case where
Âµ0 = 0
Î£0 = Ïƒ
2
0
I
so that our Gaussian prior for parameter w becomes
p

w

= N

w

 0, Ïƒ
2
0
I

here, we will go through the motions . . .
47 / 87
for the posterior, we then have
p

w

 D

âˆ p

D

 w

p

w

âˆ
Yn
j=1
N

yj

 Ï†
âŠº
j w, Ïƒ
2

Â· N

w

 0, Ïƒ
2
0
I

âˆ exp 
âˆ’
1
2Ïƒ2

Î¦âŠºw âˆ’ y


2

Â· exp 
âˆ’
1
2Ïƒ
2
0

w


2

âˆ exp 
âˆ’
1
2Ïƒ2

Î¦âŠºw âˆ’ y
âŠº

Î¦âŠºw âˆ’ y


Â· exp 
âˆ’
1
2Ïƒ
2
0
w
âŠºw

âˆ exp"
âˆ’
1
2

1
Ïƒ2

Î¦âŠºw âˆ’ y
âŠº

Î¦âŠºw âˆ’ y

+
1
Ïƒ
2
0
w
âŠºw

| {z }
=Î³
#
48 / 87
expanding Î³, we find
Î³ = w
âŠº
h
1
Ïƒ2 Î¦Î¦âŠº +
1
Ïƒ2
0
I
i
w âˆ’ 2
1
Ïƒ2 y
âŠºÎ¦âŠº w +
1
Ïƒ2 y
âŠº
y
â‡’ p

w

 D

is an exponential of a quadratic form in w
â‡” p

w

 D

is indeed another Gaussian distribution
49 / 87
further tedious yet straightforward algebra yields
p

w

 D

âˆ¼ N
w

 Âµn, Î£n

where
Î£n =
h
1
Ïƒ2 Î¦Î¦âŠº +
1
Ïƒ2
0
I
iâˆ’1
Âµn =
1
Ïƒ2 Î£n Î¦ y
50 / 87
note
â‡’ the maximum a-posteriori estimator for w is
wMAP = argmax
w
p

w

 D

=
1
Ïƒ2
h
1
Ïƒ2 Î¦Î¦âŠº +
1
Ïƒ2
0
I
iâˆ’1
Î¦ y
=
1
Ïƒ2
h
1
Ïƒ2

Î¦Î¦âŠº +
Ïƒ2
Ïƒ2
0
I

iâˆ’1
Î¦ y
=
h
Î¦Î¦âŠº +
Ïƒ2
Ïƒ2
0
I
iâˆ’1
Î¦ y
51 / 87
note
compare the ML and MAP estimators
wML =

Î¦Î¦âŠº
âˆ’1Î¦ y
wMAP =
h
Î¦Î¦âŠº +
Ïƒ2
Ïƒ2
0
I
iâˆ’1
Î¦ y
and the corresponding linear models
y = Ï†
âŠº
x wML
y = Ï†
âŠº
x wMAP
150 160 170 180 190 200
height [cm]
20
40
60
80
100
120
weight [kg]
MLE model
MAP model
52 / 87
regarding the posterior predictive distribution,
we observe that tedious calculus and algebra . . .
p

y

 x, D

=
Z
p

y

 x, D, w

p

w

 x, D

dw
=
Z
p

y

 x, w

p

w

 D

dw
=
Z
N

y

 Ï†
âŠº
x w, Ïƒ
2

Â· N

w

 Âµn, Î£n

dw
.
.
.
note: we reasonably assume
y is conditionally independent of D given x and w
w is conditionally independent of x given D
53 / 87
yield
p

y

 x, D

= N

y | Âµx, Ïƒ
2
x

where
Âµx = Ï†
âŠº
x Âµn
Ïƒ
2
x = Ïƒ
2 + Ï†
âŠº
x Î£nÏ†x
54 / 87
note
â‡’ the posterior predictive distribution for y given x and D
is a Gaussian whose mean and variance depend on x
150 160 170 180 190 200
height [cm]
20
40
60
80
100
120
weight [kg]
150
160 170 180 190 200
0
20 40 60 80 100 120
55 / 87
questions
1) what is Bayesian regression good for ?
2) does it also apply to non-linear settings ?
56 / 87
questions
1) what is Bayesian regression good for ?
2) does it also apply to non-linear settings ?
answers
1) it yields fairly â€œrobustâ€ or â€œconfidence scoredâ€ explanations of the given data
2) yes, because it does not care about the nature of the feature map we work with
letâ€™s look at a couple of examples . . .
56 / 87
example
posterior predictive distribution for Bayesian regression with 2nd order polynomial
âˆ’10 âˆ’5 0 5 10
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
p

y

 x, D

âˆ’10 âˆ’5 0 5 10
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
one-sigma plot
57 / 87
examples
âˆ’10 âˆ’5 0 5 10
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
âˆ’10 âˆ’5 0 5 10
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
âˆ’10 âˆ’5 0 5 10
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
âˆ’10 âˆ’5 0 5 10
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
âˆ’10 âˆ’5 0 5 10
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
same as above
âˆ’10 âˆ’5 0 5 10
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
missing data
âˆ’10 âˆ’5 0 5 10
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
few data
âˆ’10 âˆ’5 0 5 10
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
outliers
58 / 87
logistic regression revisited
59 / 87
binary classification
given a set of annotated training data
D =


xj
, yj
n
j=1
which consists of pairs of data points
xj âˆˆ R
m
from two classes â„¦1 and â„¦2 together
with associated class labels
yj =

+1 if xj âˆˆ â„¦1
âˆ’1 if xj âˆˆ â„¦2
60 / 87
logistic regression (classifier)
a logistic regression classifier
f

x

= tanh
x
âŠºw âˆ’ b

is being trained by estimating
wË†, Ë†b = argmin
w,b
L

w, b

where L

w, b

is the logistic loss
L

w, b

=
Xn
j=1
log
1 + e
âˆ’2yj(x
âŠº
j wâˆ’b)

61 / 87
question
but why is this called a logistic regression classifier ?
62 / 87
question
but why is this called a logistic regression classifier ?
answer
because . . .
62 / 87
for readability, we henceforth let
aj â‰¡ 2

x
âŠº
j w âˆ’ b

to simply write the logistic loss as
L =
Xn
j=1
log
1 + e
âˆ’yj aj

we further introduce new indicators zj =
1âˆ’yj
2
âˆˆ

0, 1
	
for which we note this property

âˆ’1
zj =

+1 if zj = 0
âˆ’1 if zj = 1
63 / 87
â‡’ working with the zj
, we can write
L =
Xn
j=1
log
1 + e
(âˆ’1)
z
j aj

=
X
j:zj=0
log
1 + e
aj

+
X
j:zj=1
log
1 + e
âˆ’aj

next, we recall the logistic function
Ïƒ

x

=
1
1 + eâˆ’x â‡” Ïƒ

âˆ’x

= 1 âˆ’ Ïƒ

x

from which we obtain the identities
1 + e
aj =
1
Ïƒ(âˆ’aj)
=
1
1 âˆ’ Ïƒ(aj)
and 1 + e
âˆ’aj =
1
Ïƒ(aj)
64 / 87
â‡’ working with the Ïƒ(aj), we can write
L =
X
j:zj=0
log 
1
1 âˆ’ Ïƒ(aj)

+
X
j:zj=1
log 
1
Ïƒ(aj)

=
X
j:zj=0
log
1

âˆ’ log
1 âˆ’ Ïƒ

aj


+
X
j:zj=1
log
1

âˆ’ log
Ïƒ

aj


= âˆ’ X
j:zj=0
log
1 âˆ’ Ïƒ

aj


âˆ’
X
j:zj=1
log
Ïƒ

aj


since zj âˆˆ

0, 1
	
, this can be compacted to
L = âˆ’Xn
j=1

1 âˆ’ zj

log
1 âˆ’ Ïƒ

aj


+ zj
log
Ïƒ

aj


65 / 87
note
â‡” if the aj were i.i.d. and Ïƒ(aj) with aj
7â†’ (0, 1) was a probability qj
, then the logistic loss
L = âˆ’Xn
j=1

1 âˆ’ zj

log
1 âˆ’ Ïƒ

aj


+ zj
log
Ïƒ

aj


would correspond to the negative log-likelihood for the following likelihood function
L =
Yn
j=1

1 âˆ’ qj
1âˆ’zj
Â· q
zj
j
â‡” minimizing the logistic loss L would be equivalent to maximizing the likelihood L
but can the qj = Ïƒ(aj) be interpreted as probabilities ?
66 / 87
note
if we understand the Ïƒ(aj) as conditional probabilities such that
1 âˆ’ Ïƒ

aj

= p

zj = 0

 aj

= p

yj = +1

 xj
, w, b

â‡” Ïƒ

aj

= p

zj = 1

 aj

= p

yj = âˆ’1

 xj
, w, b

we obtain a valid probabilistic interpretation of logistic regression
67 / 87
probabilistic binary classifiers model the posterior p

â„¦1

 x

= 1 âˆ’ p

â„¦2

 x

where
p

â„¦1

 x

=
p

x

 â„¦1

p

â„¦1

p

x
 =
p

x

 â„¦1

p

â„¦1

p

x

 â„¦1

p

â„¦1

+ p

x

 â„¦2

p

â„¦2

using these (elementary) identities
ab
ab + cd
=
1
ab+cd
ab
=
1
1 + cd
ab
=
1
1 + 1
ab
cd
=
1
1 + eâˆ’s
where s = log ab
cd
the posterior can also be written as
p

â„¦1

 x

=
1
1 + eâˆ’x = Ïƒ

x

where x = log
p

x

 â„¦1

p

â„¦1

p

x

 â„¦2

p

â„¦2
 = log p

x

 â„¦1

p

â„¦1

âˆ’ log p

x

 â„¦2

p

â„¦2

â‡’ a logistic regression classifier models the abstract expression log p

x

 â„¦1

p

â„¦1

âˆ’ log p

x

 â„¦2

p

â„¦2

in terms of x
âŠºw âˆ’ b
68 / 87
a hard probabilistic binary classifier f : R
m â†’ {Â±1} computes
f

x

=

+1 if p

â„¦1

 x

â©¾ p

â„¦1

 x

âˆ’1 otherwise
however, if we are already going through the hassle of modeling
and evaluating the posteriors p

x

 â„¦1

and p

x

 â„¦2

, we may
as well predict class probability vectors
q âˆˆ âˆ†1 =


x âˆˆ R
2


 x â©¾ 0 âˆ§ 1
âŠºx = 1

â‡’ we might as well consider a classifier f : R
m â†’ âˆ†1
that computes
f

x

=

q1
q2

=

p

â„¦1

 x

p

â„¦2

 x


âˆ†kâˆ’1
is the standard simplex in R
k
it will feature prominently later on . . .
69 / 87
principle of machine learning
instead of directly predicting class labels for K â©¾ 2 classes,
it may be better to predict (a vector of) class probabilities
f

x

=
ï£®
ï£¯
ï£¯
ï£¯
ï£°
p

â„¦1

 x

p

â„¦2

 x

.
.
.
p

â„¦K

 x

ï£¹
ï£º
ï£º
ï£º
ï£»
such probabilities can be turned into decisions by using, say
kâˆ— = argmax
k
h
f

x

i
k
= argmax
k
e
âŠº
k
f

x

70 / 87
principle of machine learning
when training probabilistic classifiers, we do not work with class labels such as
yj âˆˆ

0, 1
	
or yj âˆˆ

âˆ’1, +1
	
or yj âˆˆ

1, 2, . . . , K
	
instead, it is much better (more convenient) to consider class label vectors like
yj =

0
1

or yj =
ï£®
ï£°
0
1
0
ï£¹
ï£» or even yj =
ï£®
ï£°
1/4
1/2
1/4
ï£¹
ï£»
71 / 87
note
all the preceding examples considered label vectors yj âˆˆ âˆ†
Kâˆ’1
such vectors are called probability vectors or stochastic vectors
in classification tasks, their entries represent class probabilities

yj

i
= p

â„¦i

 xj

the binary variables zj âˆˆ {0, 1} which we worked with above can
be understood as entries of stochastic vectors zj âˆˆ âˆ†
1
, namely
zj =

1 âˆ’ zj
zj

=

p

â„¦1

 xj

p

â„¦2

 xj


72 / 87
cross entropy
letting y,Ë†y âˆˆ âˆ†
Kâˆ’1 be two probability vectors / discrete distributions, the cross entropy
H

y,Ë†y

= âˆ’Ey

log Ë†y

= âˆ’X
K
k=1
yk log Ë†yk
of Ë†y relative to y is an information theoretic measure of much these distributions agree
â‡” if y is a target and Ë†y is a prediction (such as produced by a classifier), then the cross
entropy H

y,Ë†y

is a (possible) measure of the quality of the prediction
73 / 87
note
we generally have asymmetry H

y,Ë†y

Ì¸= H

Ë†y, y

â‡” (cross) entropies are not distances
machine learning practitioners usually consider H

y,Ë†y + Ïµ

where Ïµ â‰ª 1,
(e.g. Ïµ = 10âˆ’8
) to avoid problems with Ë†yk = 0 for which log Ë†yk is undefined
yes, this is a quick and dirty and altogether brutal fix but common practice
theoreticians would argue that Ë†yk = 0 indicates a zero probability which can be excluded from computations and compute
H

y,Ë†y

= âˆ’ X
k:Ë†y
k
Ì¸=0
yk
log Ë†yk
74 / 87
note
in our analysis of the logistic loss function, we used probability vectors
zj =

1 âˆ’ zj
zj

and qj =

1 âˆ’ qj
qj

recalling that the j-th term Lj of the logistic loss function L is given by
Lj = âˆ’
1 âˆ’ zj

log
1 âˆ’ qj

âˆ’ zj
log
qj

= âˆ’
K
X=2
k=1

zj

k
log
qj

k
we now realize that this quantity is the cross entropy between a target
distribution (zj) and a predicted distribution which results from training
a classifier ( qj
)
75 / 87
note
â‡’ we realize that
the logistic loss function
L

w, b

=
Xn
j=1
log
1 + e
âˆ’2yj(x
âŠº
j wâˆ’b)

used in binary classifier training with xj âˆˆ R
m
and yj âˆˆ {Â±1} is a sum of cross entropies
76 / 87
one more thing . . .
in lecture 04, we considered the logistic loss
to train a binary classifier of the form
f

x

= h

a

x
 = tanh
x
âŠºw âˆ’ b

now, we note that tanh used to be a popular
activation function for neural networks
next, we shall show that tanh-neurons are
Bayesian decision makers
âˆ’1 x1 x2 Â· Â· Â· xm
h(a)
y
b w1 w2
wm
77 / 87
to see how tanh(a) relates to Bayesian decision making,
we reconsider the binary classification problem
assign a âˆˆ R to either one of two classes â„¦1 or â„¦2
a Bayesian classifier for this problem is given by
â„¦

a

=

â„¦1 if p

â„¦1

 a

â©¾ p

â„¦2

 a

â„¦2 otherwise
where
p

â„¦i

 a

=
p

a

 â„¦i

Â· p

â„¦i

p

a
 =
p

a, â„¦i

p

a, â„¦1

+ p

a, â„¦2

78 / 87
next, we make three didactic assumptions
1) the priors of both classes are equal
p

â„¦i

=
1
2
2) their likelihoods follow normal distributions
p

a

 â„¦i

= âˆš
1
2Ï€Ïƒ2
i
e
âˆ’
(aâˆ’Âµi)
2
2Ïƒ2
i
3) these are mirror symmetric about a = 0 so that
+Âµ1 = âˆ’Âµ2 = Âµ
Ïƒ
2
1 = Ïƒ
2
2 = Ïƒ
2
79 / 87
â‡’ we consider joint distributions
p

a, â„¦1

=
1
2
âˆš
1
2Ï€Ïƒ2
e
âˆ’ a
2
2Ïƒ2 e
âˆ’
Âµ2
2Ïƒ2 e
+
a Âµ
Ïƒ2
p

a, â„¦2

=
1
2
âˆš
1
2Ï€Ïƒ2
e
âˆ’ a
2
2Ïƒ2 e
âˆ’
Âµ2
2Ïƒ2 e
âˆ’
a Âµ
Ïƒ2
âˆ’Âµ +Âµ
1
p(a, â„¦2) p(a, â„¦1)
80 / 87
â‡’ we have the following posteriors
p

â„¦1

 a

=
e
+
a Âµ
Ïƒ2
e
+
a Âµ
Ïƒ2 + e
âˆ’
a Âµ
Ïƒ2
p

â„¦2

 a

=
e
âˆ’
a Âµ
Ïƒ2
e
+
a Âµ
Ïƒ2 + e
âˆ’
a Âµ
Ïƒ2
âˆ’Âµ +Âµ
1
p(â„¦1 | a) = p(a,â„¦1)
p(a) p(â„¦2 | a) = p(a,â„¦2)
p(a)
81 / 87
now, we observe
p

â„¦1

 a

â©¾ p

â„¦2

 a

â‡” p

â„¦1

 a

âˆ’ p

â„¦2

 a

â©¾ 0
p

â„¦1

 a

âˆ’ p

â„¦2

 a

=
e
+
a Âµ
Ïƒ2 âˆ’ e
âˆ’
a Âµ
Ïƒ2
e
+
a Âµ
Ïƒ2 + e
âˆ’
a Âµ
Ïƒ2
âˆ’Âµ +Âµ
-1
1
p(â„¦1 | a) âˆ’ p(â„¦2 | a)
82 / 87
â‡’ our Bayesian classifier computes
â„¦

a

=

â„¦1 if p

â„¦1

 a

â©¾ p

â„¦2

 a

â„¦2 otherwise
=

â„¦1 if tanh
Î² a

â©¾ 0
â„¦2 otherwise
where
Î² =
Âµ
Ïƒ2
83 / 87
note
â‡’ when a neuron computes
f

x

= h

a

x
 = tanh
x
âŠºw âˆ’ b

it maps x âˆˆ R
m to a = w
âŠºx âˆ’ b âˆˆ R and then evaluates the difference of
two posterior distributions whose corresponding likelihoods are simple
Gaussians (with means b + Âµ, b âˆ’ Âµ and variances Ïƒ
2 = Âµ)
84 / 87
summary
85 / 87
we now know about
maximum likelihood regression as well as Bayesian regression
the fact that (ordinary) least squares regression is the same as
maximum likelihood regression with a Gaussian noise model
â‡” the fact that a least squared error is actually a (scaled) variance
probabilistic interpretations of- or models for (binary) classification
the fact that logistic losses are sums of cross entropies in disguise
the fact that even tanh activations allow for a Bayesian interpretation
turn page for final words of wisdom . . .
86 / 87
Bayesian inference appears to be cumbersome and tedious
indeed, our lectures only scratched the surface of the idea
â€œreal worldâ€ Bayesian inference is often very, very involved
however . . .
there also are so called variational Bayes techniques
in particular, stochastic variational Bayes techniques
these enable Bayesian inference with very large models (millions of parameters)
indeed, so called variational autoencoders have become a popular model class
and allow for Bayesian backpropagation to train (recurrent) neural networks . . .
87 / 87